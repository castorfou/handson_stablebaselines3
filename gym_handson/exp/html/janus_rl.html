<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>janus_rl API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>janus_rl</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 04 - refactoring Janus and gym.ipynb


import gym
from gym import spaces
import random
import stable_baselines3
from stable_baselines3.common.env_checker import check_env
from stable_baselines3 import DQN, DDPG
from math import sqrt
import numpy as np
import pandas as pd
from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn import preprocessing
from sklearn.datasets import make_regression
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
import math
import pickle

class Janus(gym.Env):
    metadata = {&#39;render.modes&#39;: [&#39;human&#39;]}
    template_filename = &#39;data/dataset-S_public/public/dataset_S-{}.csv&#39;

    def __init__(self, idx=47, reward_function=&#39;clown_hat&#39;):
        &#39;&#39;&#39;
        idx: index of observation file (full_x 13000 dropouts), default = 47
        reward_function: string among [&#39;clown_hat&#39;, &#39;archery&#39;, &#39;smart_archery&#39;], default &#39;clown_hat&#39;
        &#39;&#39;&#39;
        super(Janus, self).__init__()
        #actions: move on the grid, by continuous value in -1,1
        #0,0 no move
        #based on 94 controlable parameters
        #&#34;We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) &#34;, we will multiply effect by 2
#         self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(94, ))
        #we focus on the 1 most influencal action
        nbr_actions = 4
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(nbr_actions, ))


        # all the observation_space
        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(228, ))

        file1 = pd.read_csv(self.template_filename.format(&#39;file1&#39;), index_col=0)
        file2 = pd.read_csv(self.template_filename.format(&#39;file2&#39;), index_col=0)
        file3 = pd.read_csv(self.template_filename.format(&#39;file3&#39;), index_col=0)
        vav = pd.read_csv(self.template_filename.format(&#39;VAV&#39;), index_col=0)
        self.ti = pd.read_csv(self.template_filename.format(&#39;TI&#39;), index_col=0)
        self.ts = pd.read_csv(self.template_filename.format(&#39;TS&#39;), index_col=0)
        x_df = file1.copy()
        x_df = x_df.loc[:, (x_df != 0).any(axis=0)]  ## remove static columns
        x_df = x_df.fillna(x_df.mean())  ## replace nan with mean

        self.y_df = file2.copy()
        self.y_df.dropna(how=&#39;all&#39;, axis=1,
                    inplace=True)  ## drop full tempty columns
        self.y_df = self.y_df.fillna(self.y_df.mean())

        self.vav_df = vav.copy()
        # Dropping few columns
        for dataset in [self.y_df, self.vav_df, self.ti, self.ts]:
            dataset.drop([&#39;target_1&#39;, &#39;target_2&#39;, &#39;target_3&#39;, &#39;target_4&#39;],
                  axis=1,
                  inplace=True)  #to simplify with a 2-dimension target space

        print(&#39;features shape: {}, \ntargets shape: {}&#39;.format(
            x_df.shape, self.y_df.shape))

        x_train, x_test, y_train, y_test = train_test_split(x_df,
                                                            self.y_df,
                                                            test_size=0.1,
                                                            random_state=14)
        print(&#39;\nLength of train is {}, test is {}&#39;.format(
            len(x_train), len(x_test)))
        ## Random forest
        filename = &#39;data/models/janus_RF.pkl&#39;  # janus_LinearReg, janus_RF

        # pickle.dump(ml_model, open(filename, &#39;wb&#39;))

        # # load the model from disk
        self.ml_model = pickle.load(open(filename, &#39;rb&#39;))
        print(f&#39;R squared: {self.ml_model.score(x_test, y_test.values):0.04f}&#39;)

        self.full_x = file3.copy()[x_df.columns]
        self.full_x = self.full_x.fillna(x_df.mean())

        self.partial_x = x_train.copy()

        inferred_y = pd.DataFrame(self.ml_model.predict(self.full_x),
                                  columns=self.y_df.columns)

        # list of [min, max, step, range] for each var
        scale = 100
        decimals = 3

        self.list_important_actions = np.argsort(self.ml_model.feature_importances_[:94])[::-1][:nbr_actions]

        ## get limits for Rewards
        self.output_steps = [round((self.y_df[i].max() - self.y_df[i].min())/scale, decimals) \
                        for i in self.y_df.columns]
        print(&#39;Output steps: &#39;, self.output_steps)

        self.idx=idx

        assert reward_function in [&#39;clown_hat&#39;, &#39;archery&#39;, &#39;smart_archery&#39;], reward_function
        self.reward_function = reward_function
        print(f&#39;Active reward function {self.reward_function}&#39;)

    def reset(self):
#         self.current_position = self.revert_to_obs_space(
#             self.full_x.sample().values.reshape(-1), self.full_x)
#         random.seed(13)
#         idx = random.randint(0,len(janus_env.partial_x)-1)
        self.current_position = self.revert_to_obs_space(
            self.full_x.iloc[self.idx].values.reshape(-1), self.full_x)


        # to fix The observation returned by the `reset()` method does not match the given observation space
        # maybe won&#39;t happen on linux
        # on windows looks like float64 is the defautl for pandas -&gt; numpy and gym expects float32 (contains tries to cast to dtype(float32))
        self.current_position = self.current_position.astype(&#39;float32&#39;)

        self.last_action = self.current_position[self.list_important_actions]
        self.last_effect = False
        self.global_reward = 0
        self.episode_length = 0
        #print(f&#39;reset at position {self.current_position[:10]}...&#39;)
        return self.current_position

    def step(self, action):
#         self.current_position[0:len(action)] = action
        for index, act in enumerate(self.list_important_actions):
            self.current_position[act]=action[index]
        self.last_action = action
        self.episode_length += 1
        done = False

        reward = self.reward(
            self.convert_to_real_obs(self.current_position,
                                     self.full_x).values.reshape(1,-1))
        if (reward &gt;= -0.1*self.y_df.shape[1]):
            done = True

#         print(f&#39;target reached {done} reward {reward:0.03f} n° step {self.episode_length} action {self.last_action} done threeshold {-0.1*self.y_df.shape[1]:0.03f}&#39;)

        if done:
            reward += 100

        if self.episode_length&gt;100:
            print(&#39;episode too long -&gt; reset&#39;)
            done = True

#         if (max(abs(action))==1):
#             # if on border, we kill episode
#             print(&#39;border reached -&gt; done -&gt; reset&#39;)
#             reward -= 50
#             done = True


        self.global_reward += reward
#         print(f&#39;  reward {reward:0.03f} global reward {self.global_reward:0.03f} done {done} action {action} step {self.episode_length}&#39;)
        return self.current_position, reward, done, {}

    def render(self):
        print(
            f&#39;position {self.current_position[:10]}, action {self.last_action[:5]}, effect {self.last_effect}, done {done}, global_reward {self.global_reward:0.03f}&#39;
        )

    def convert_to_real_obs(self, observation, observation_dataset):
        &#39;&#39;&#39;
        to convert an observation from observation space ([-1, 1],325) to  real world
        -1 matches with min() of each column
        1 matches with max() of each column

        observation: instance of observation_space
        observation_dataset: the full real dataset (obfuscated in that case)
        &#39;&#39;&#39;
        return (observation + np.ones(self.observation_space.shape)) / 2 * (
            observation_dataset.max() -
            observation_dataset.min()) + observation_dataset.min()

    def revert_to_obs_space(self, real_observation, observation_dataset):
        &#39;&#39;&#39;
        to revert an observation sample (from real world) to observation space
        min() of each column will match with -1
        max() of each column will match with +1

        real_observation: instance of real_world
        observation_dataset: the full real dataset (obfuscated in that case)
        &#39;&#39;&#39;
        return np.nan_to_num(
            2 * (real_observation - observation_dataset.min()) /
            (observation_dataset.max() - observation_dataset.min()) -
            np.ones(self.observation_space.shape)).reshape(-1)

    def reward(self, observation):
        &#39;&#39;&#39;reward
        observation is from real world not observation space
        map reward function according to
        &#39;&#39;&#39;

        reward_function = self.reward_clown_hat
        if (self.reward_function == &#39;archery&#39;):
            reward_function = self.reward_archery
        if (self.reward_function == &#39;smart_archery&#39;):
            reward_function = self.reward_smart_archery

        new_y = self.ml_model.predict(observation).reshape(-1)
#         return self.continuous_reward_clown_hat(new_y)
        return reward_function(new_y)

    def discrete_reward(self, new_y):
        &#39;&#39;&#39; Discrete reward &#39;&#39;&#39;

        new_val = [
            sqrt((self.vav_df.iloc[:, i].values[0] - new_y[i])**2)
            for i in range(len(new_y))
        ]
        k = 10
        k1 = 1
        if new_val[0] &lt; k * self.output_steps[0] and new_val[
                1] &lt; k * self.output_steps[1]:
            reward = 1  #dans les 10% d&#39;amplitude max autour de la vav
            if new_val[0] &lt; k1 * self.output_steps[0] and new_val[
                    1] &lt; k1 * self.output_steps[1]:
                reward = 10  #dans les 1% d&#39;amplitude max autour de la vav
                on_target = True
#                 print(&#39;On Target : &#39;, new_y)
        else:
            reward = -1
        return reward



    def reward_clown_hat(self, new_y):
        &#39;&#39;&#39; clown_hat reward &#39;&#39;&#39;
        final_reward = 0

        for i in range(len(new_y)):
            reward = -9
            if ( self.ti.iloc[:,i].values[0] &lt;=  new_y[i] &lt;= self.ts.iloc[:,i].values[0]):
                if ( new_y[i] &gt;= self.vav_df.iloc[:,i].values[0] ):
                    reward = 1-(new_y[i]-self.vav_df.iloc[:,i].values[0])/(self.ts.iloc[:,i].values[0]-self.vav_df.iloc[:,i].values[0])
                else:
                    reward = 1-(self.vav_df.iloc[:,i].values[0]-new_y[i])/(self.vav_df.iloc[:,i].values[0]-self.ti.iloc[:,i].values[0])
            reward += -1
            final_reward+=reward
    #         print(f&#39;reward {reward} final_reward {final_reward} i {i}&#39;)

        if (final_reward&gt;0.7*len(new_y)):
            on_target = True
    #         print(&#39;On Target : &#39;, new_y)

        return final_reward

    def reward_archery(self, new_y):
        &#39;&#39;&#39; archery reward
        drawback is that you need progress on all targets to get reward improvment
        &#39;&#39;&#39;
        final_reward = 0

        ti_target_0 = self.ti.iloc[:,0].values[0]
        ts_target_0 = self.ts.iloc[:,0].values[0]
        ti_target_5 = self.ti.iloc[:,1].values[0]
        ts_target_5 = self.ts.iloc[:,1].values[0]
        x, y = new_y[0], new_y[1]
        if ( (ti_target_0*0.10 &lt;= x &lt;= ts_target_0*0.10) &amp; ( ti_target_5*0.10 &lt;= y &lt;= ts_target_5*0.10 )):
            reward = 0
        else:
            if ( (ti_target_0*0.50 &lt;= x &lt;= ts_target_0*0.50) &amp; ( ti_target_5*0.50 &lt;= y &lt;= ts_target_5*0.50 )):
                reward = -2
            else:
                if ( (ti_target_0 &lt;= x &lt;= ts_target_0) &amp; ( ti_target_5 &lt;= y &lt;= ts_target_5 )):
                    reward = -5
                else:
                    reward = -20
        final_reward = reward
        return final_reward

    def reward_smart_archery(self, new_y):
        &#39;&#39;&#39; smart archery reward &#39;&#39;&#39;
        ti_target_0 = self.ti.iloc[:,0].values[0]
        ts_target_0 = self.ts.iloc[:,0].values[0]
        ti_target_5 = self.ti.iloc[:,1].values[0]
        ts_target_5 = self.ts.iloc[:,1].values[0]
        x, y = new_y[0], new_y[1]

        reward_x = 0
        if (x &lt;= ti_target_0 or x &gt;= ts_target_0): reward_x = -10
        if ( ti_target_0 &lt;= x &lt;= 0.5*ti_target_0  or 0.5*ts_target_0 &lt;= x &lt;= ts_target_0  ): reward_x = -3
        if ( 0.5*ti_target_0 &lt;= x &lt;= 0.1*ti_target_0  or 0.1*ts_target_0 &lt;= x &lt;= 0.5*ts_target_0  ): reward_x = -1
        reward_y = 0
        if (y &lt;= ti_target_5 or y &gt;= ts_target_5): reward_y = -10
        if ( ti_target_5 &lt;= y &lt;= 0.5*ti_target_5  or 0.5*ts_target_5 &lt;= y &lt;= ts_target_5  ): reward_y = -3
        if ( 0.5*ti_target_5 &lt;= y &lt;= 0.1*ti_target_5  or 0.1*ts_target_5 &lt;= y &lt;= 0.5*ts_target_5  ): reward_y = -1

        final_reward = reward_x + reward_y
        return final_reward



from stable_baselines3.common.env_checker import check_env

janus_env = Janus()

check_env(janus_env)


from stable_baselines3 import TD3
from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise
import warnings
warnings.filterwarnings(&#39;ignore&#39;)

def train_RL_model(exp_number, idx, reward_name):
    &#39;&#39;&#39;
    train a TD3 model with OrnsteinUhlenbeckActionNoise
    10000 timesteps
    starting with observation $idx$
    and log it on tensorboard within entry: EXP$exp_number$ - IDX$idx - $reward_name
    &#39;&#39;&#39;
    janus = Janus(idx, reward_name)
    check_env(janus)

    model_name = &#34;EXP {} - IDX {} - {}&#34;.format(exp_number, idx, reward_name)
    n_actions = janus.action_space.shape[-1]
    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))
    action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))

    model_janus_td3 = TD3(&#34;MlpPolicy&#34;, janus, action_noise=action_noise, verbose=2,tensorboard_log=&#34;./tensorboard/&#34;)
    model_janus_td3.learn(total_timesteps=10000, log_interval=4, tb_log_name=model_name)
    model_janus_td3.save(&#34;./data/&#34;+model_name)


from stable_baselines3 import TD3
from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise

def load_RL_model(exp_number, idx, reward_name):
    &#39;&#39;&#39;
    load a pre-trained TD3 model with OrnsteinUhlenbeckActionNoise
    from data/EXP$exp_number$ - IDX$idx - $reward_name
    &#39;&#39;&#39;
    janus = Janus(idx, reward_name)
    check_env(janus)
    janus.reset()

    model_name = &#34;EXP {} - IDX {} - {}&#34;.format(exp_number, idx, reward_name)
    print(f&#39;model used: {model_name}&#39;)

    n_actions = janus.action_space.shape[-1]
    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))
    action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))
    model_janus_td3 = TD3(&#34;MlpPolicy&#34;, janus, action_noise=action_noise, verbose=2,tensorboard_log=&#34;./tensorboard/&#34;)
    model_janus_td3.load(&#34;./data/&#34;+model_name)

    return model_janus_td3



janus_env = Janus()
inf_x = janus_env.ti.iloc[0][0]
sup_x = janus_env.ts.iloc[0][0]
inf_y = janus_env.ti.iloc[0][1]
sup_y = janus_env.ts.iloc[0][1]
inf_limx = min(inf_x, janus_env.y_df.min()[0])
sup_limx = max(sup_x, janus_env.y_df.max()[0])
inf_limy = min(inf_y, janus_env.y_df.min()[1])
sup_limy = max(sup_y, janus_env.y_df.max()[1])
vav_x = janus_env.vav_df.iloc[0].values[0]
vav_y = janus_env.vav_df.iloc[0].values[1]

def calculate_reward_outputs(reward=janus_env.reward_clown_hat, reward_name = &#39;clown_hat&#39;):
    &#39;&#39;&#39;
    take a grid of all possible values for target_0, target_5 (between [min(target) - 3, max(target)+3]
    and calculate reward values
    keep that in a dataframe with columns [&#39;x&#39;, &#39;y&#39;, &#39;z&#39;]
    at /data/reward_$reward_name$.csv
    &#39;&#39;&#39;

    X = np.arange(inf_limx-3, sup_limx+3, 0.2)
    Y = np.arange(inf_limy-3, sup_limy+3, 0.2)
    xyz_content=[]
    for x in X:
        for y in Y:
            xyz_content.append([x, y, reward([x,y])])
    xyz_content = pd.DataFrame(xyz_content, columns=[&#39;x&#39;, &#39;y&#39;, &#39;z&#39;])
    xyz_content.to_csv(&#39;./data/reward_&#39;+reward_name+&#39;.csv&#39;)


import plotly.express as px

def plot_reward(reward_name):
    &#39;&#39;&#39;
    3D plot with plotly (with interesting interactivity)
    based on csv file /data/reward_$reward_name$.csv

    if plotly graph is not displayed, check if notebook is trusted on top right of the screen.
    &#39;&#39;&#39;
    xyz_content = pd.read_csv(&#39;./data/reward_&#39;+reward_name+&#39;.csv&#39;, index_col=0)
    fig = px.scatter_3d(xyz_content, x=&#39;x&#39;, y=&#39;y&#39;, z=&#39;z&#39;,
                  color=&#39;z&#39;)
    fig.show()


def calculate_reward_dropouts(reward=janus_env.reward_clown_hat, reward_name = &#39;clown_hat&#39;):
    &#39;&#39;&#39;
    consider all observations available (all dropouts)
    predict quality output
    calculate each reward (one reward per dropout)
    keep that in a dataframe with columns [&#39;target_0&#39;, &#39;target_5&#39;, &#39;reward_&#39;+reward_name]
    at /data/full_x_$reward_name$.csv
    &#39;&#39;&#39;

    full_prediction = janus_env.ml_model.predict(janus_env.full_x)
    reward_df = pd.DataFrame(full_prediction, columns=[&#39;target_0&#39;, &#39;target_5&#39;])

    reward_df[&#39;reward_&#39;+reward_name]=reward_df.apply(lambda x: reward([x[0], x[1]]), axis=1)
    reward_df.to_csv(&#39;./data/full_x_&#39;+reward_name+&#39;.csv&#39;)



from plotly.subplots import make_subplots
import plotly.graph_objects as go


def plot_reward_of_dropouts(reward_name):
    &#39;&#39;&#39;
    from full_x_$reward_name$.csv file, plot scatter + histogram of rewards
    &#39;&#39;&#39;

    reward_df=pd.read_csv(&#39;./data/full_x_&#39;+reward_name+&#39;.csv&#39;, index_col=0)
    fig = make_subplots(rows=2, cols=1)

    for col in reward_df.columns:
        fig.add_trace(go.Scatter(
            x=reward_df.index,
            y=reward_df[col], showlegend=True, name=col),  row=1, col=1)
    fig.add_trace(go.Histogram(x=reward_df[reward_df.columns[-1]], name = reward_df.columns[-1], marker=dict(color=px.colors.qualitative.Plotly[2])), row=2, col=1)

    fig.update_layout(
        width=1000,
        height=800)

    fig.show()



import plotly.graph_objects as go

def positionne_point_idx(janus_env, model, fig):
    print(f&#39;Index de l observation {janus_env.idx}&#39;)
    current_position = janus_env.revert_to_obs_space(janus_env.full_x.iloc[janus_env.idx].values.reshape(-1), janus_env.full_x)
    current_position = current_position.astype(&#39;float32&#39;)
    current_y = janus_env.ml_model.predict(janus_env.convert_to_real_obs(current_position,
                                     janus_env.full_x).values.reshape(1,-1)).reshape(-1)
    print(f&#39;init results:  {current_y} reward {janus_env.reward(janus_env.convert_to_real_obs(janus_env.current_position, janus_env.full_x).values.reshape(1,-1)):0.03f} action {janus_env.last_action}&#39;)
    fig.add_trace(go.Scatter(x=[current_y[0]], y=[current_y[1]], name=&#39;point initial obs &#39;+str(janus_env.idx)))


    for i in range(100):
        action, _ = model.predict(janus_env.current_position)
    #     print(f&#39;action {action}&#39;)
        obs, rewards, done, info = janus_env.step(action)
    #     janus_env.render()
        if done:
            print(f&#39;done within {i+1} iterations&#39;)
            break
    new_y = janus_env.ml_model.predict(janus_env.convert_to_real_obs(janus_env.current_position,
                                         janus_env.full_x).values.reshape(1,-1)).reshape(-1)

    print(f&#39;final results:  {new_y} reward {janus_env.reward(janus_env.convert_to_real_obs(janus_env.current_position, janus_env.full_x).values.reshape(1,-1)):0.03f} action {janus_env.last_action}&#39;)
    fig.add_trace(go.Scatter(x=[new_y[0]], y=[new_y[1]], name=&#39;point final obs &#39;+str(janus_env.idx)))

def visualise_avant_apres(exp_number, idx, reward_name):
    &#39;&#39;&#39;
    create janus env(idx, reward_name)
    load rl model()
    plot initial action and prescripted one after
    &#39;&#39;&#39;
    fig = go.Figure()

    janus = Janus(idx, reward_name)
    check_env(janus)
    janus.reset()
    RL_model = load_RL_model(exp_number, idx, reward_name)

    fig.update_xaxes(title_text=&#39;target_0&#39;, range=[inf_limx-3, sup_limx+3])
    fig.update_yaxes(title_text=&#39;target_5&#39;, range=[inf_limy-3, sup_limy+3])

    fig.add_trace(go.Scatter(x=[inf_x,inf_x,sup_x,sup_x, inf_x, None, vav_x], y=[inf_y,sup_y,sup_y, inf_y, inf_y, None, vav_y], fill=&#34;toself&#34;, name=&#39;TI, TS, VAV&#39;))
    positionne_point_idx(janus, RL_model, fig)
    fig.update_layout({&#39;showlegend&#39;: True, &#39;title&#39;:&#39;Position de la qualité, avant, après&#39;})
    fig.show()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="janus_rl.calculate_reward_dropouts"><code class="name flex">
<span>def <span class="ident">calculate_reward_dropouts</span></span>(<span>reward=&lt;bound method Janus.reward_clown_hat of &lt;janus_rl.Janus object&gt;&gt;, reward_name='clown_hat')</span>
</code></dt>
<dd>
<div class="desc"><p>consider all observations available (all dropouts)
predict quality output
calculate each reward (one reward per dropout)
keep that in a dataframe with columns ['target_0', 'target_5', 'reward_'+reward_name]
at /data/full_x_$reward_name$.csv</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_reward_dropouts(reward=janus_env.reward_clown_hat, reward_name = &#39;clown_hat&#39;):
    &#39;&#39;&#39;
    consider all observations available (all dropouts)
    predict quality output
    calculate each reward (one reward per dropout)
    keep that in a dataframe with columns [&#39;target_0&#39;, &#39;target_5&#39;, &#39;reward_&#39;+reward_name]
    at /data/full_x_$reward_name$.csv
    &#39;&#39;&#39;

    full_prediction = janus_env.ml_model.predict(janus_env.full_x)
    reward_df = pd.DataFrame(full_prediction, columns=[&#39;target_0&#39;, &#39;target_5&#39;])

    reward_df[&#39;reward_&#39;+reward_name]=reward_df.apply(lambda x: reward([x[0], x[1]]), axis=1)
    reward_df.to_csv(&#39;./data/full_x_&#39;+reward_name+&#39;.csv&#39;)</code></pre>
</details>
</dd>
<dt id="janus_rl.calculate_reward_outputs"><code class="name flex">
<span>def <span class="ident">calculate_reward_outputs</span></span>(<span>reward=&lt;bound method Janus.reward_clown_hat of &lt;janus_rl.Janus object&gt;&gt;, reward_name='clown_hat')</span>
</code></dt>
<dd>
<div class="desc"><p>take a grid of all possible values for target_0, target_5 (between [min(target) - 3, max(target)+3]
and calculate reward values
keep that in a dataframe with columns ['x', 'y', 'z']
at /data/reward_$reward_name$.csv</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_reward_outputs(reward=janus_env.reward_clown_hat, reward_name = &#39;clown_hat&#39;):
    &#39;&#39;&#39;
    take a grid of all possible values for target_0, target_5 (between [min(target) - 3, max(target)+3]
    and calculate reward values
    keep that in a dataframe with columns [&#39;x&#39;, &#39;y&#39;, &#39;z&#39;]
    at /data/reward_$reward_name$.csv
    &#39;&#39;&#39;

    X = np.arange(inf_limx-3, sup_limx+3, 0.2)
    Y = np.arange(inf_limy-3, sup_limy+3, 0.2)
    xyz_content=[]
    for x in X:
        for y in Y:
            xyz_content.append([x, y, reward([x,y])])
    xyz_content = pd.DataFrame(xyz_content, columns=[&#39;x&#39;, &#39;y&#39;, &#39;z&#39;])
    xyz_content.to_csv(&#39;./data/reward_&#39;+reward_name+&#39;.csv&#39;)</code></pre>
</details>
</dd>
<dt id="janus_rl.load_RL_model"><code class="name flex">
<span>def <span class="ident">load_RL_model</span></span>(<span>exp_number, idx, reward_name)</span>
</code></dt>
<dd>
<div class="desc"><p>load a pre-trained TD3 model with OrnsteinUhlenbeckActionNoise
from data/EXP$exp_number$ - IDX$idx - $reward_name</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_RL_model(exp_number, idx, reward_name):
    &#39;&#39;&#39;
    load a pre-trained TD3 model with OrnsteinUhlenbeckActionNoise
    from data/EXP$exp_number$ - IDX$idx - $reward_name
    &#39;&#39;&#39;
    janus = Janus(idx, reward_name)
    check_env(janus)
    janus.reset()

    model_name = &#34;EXP {} - IDX {} - {}&#34;.format(exp_number, idx, reward_name)
    print(f&#39;model used: {model_name}&#39;)

    n_actions = janus.action_space.shape[-1]
    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))
    action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))
    model_janus_td3 = TD3(&#34;MlpPolicy&#34;, janus, action_noise=action_noise, verbose=2,tensorboard_log=&#34;./tensorboard/&#34;)
    model_janus_td3.load(&#34;./data/&#34;+model_name)

    return model_janus_td3</code></pre>
</details>
</dd>
<dt id="janus_rl.plot_reward"><code class="name flex">
<span>def <span class="ident">plot_reward</span></span>(<span>reward_name)</span>
</code></dt>
<dd>
<div class="desc"><p>3D plot with plotly (with interesting interactivity)
based on csv file /data/reward_$reward_name$.csv</p>
<p>if plotly graph is not displayed, check if notebook is trusted on top right of the screen.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_reward(reward_name):
    &#39;&#39;&#39;
    3D plot with plotly (with interesting interactivity)
    based on csv file /data/reward_$reward_name$.csv

    if plotly graph is not displayed, check if notebook is trusted on top right of the screen.
    &#39;&#39;&#39;
    xyz_content = pd.read_csv(&#39;./data/reward_&#39;+reward_name+&#39;.csv&#39;, index_col=0)
    fig = px.scatter_3d(xyz_content, x=&#39;x&#39;, y=&#39;y&#39;, z=&#39;z&#39;,
                  color=&#39;z&#39;)
    fig.show()</code></pre>
</details>
</dd>
<dt id="janus_rl.plot_reward_of_dropouts"><code class="name flex">
<span>def <span class="ident">plot_reward_of_dropouts</span></span>(<span>reward_name)</span>
</code></dt>
<dd>
<div class="desc"><p>from full_x_$reward_name$.csv file, plot scatter + histogram of rewards</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_reward_of_dropouts(reward_name):
    &#39;&#39;&#39;
    from full_x_$reward_name$.csv file, plot scatter + histogram of rewards
    &#39;&#39;&#39;

    reward_df=pd.read_csv(&#39;./data/full_x_&#39;+reward_name+&#39;.csv&#39;, index_col=0)
    fig = make_subplots(rows=2, cols=1)

    for col in reward_df.columns:
        fig.add_trace(go.Scatter(
            x=reward_df.index,
            y=reward_df[col], showlegend=True, name=col),  row=1, col=1)
    fig.add_trace(go.Histogram(x=reward_df[reward_df.columns[-1]], name = reward_df.columns[-1], marker=dict(color=px.colors.qualitative.Plotly[2])), row=2, col=1)

    fig.update_layout(
        width=1000,
        height=800)

    fig.show()</code></pre>
</details>
</dd>
<dt id="janus_rl.positionne_point_idx"><code class="name flex">
<span>def <span class="ident">positionne_point_idx</span></span>(<span>janus_env, model, fig)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def positionne_point_idx(janus_env, model, fig):
    print(f&#39;Index de l observation {janus_env.idx}&#39;)
    current_position = janus_env.revert_to_obs_space(janus_env.full_x.iloc[janus_env.idx].values.reshape(-1), janus_env.full_x)
    current_position = current_position.astype(&#39;float32&#39;)
    current_y = janus_env.ml_model.predict(janus_env.convert_to_real_obs(current_position,
                                     janus_env.full_x).values.reshape(1,-1)).reshape(-1)
    print(f&#39;init results:  {current_y} reward {janus_env.reward(janus_env.convert_to_real_obs(janus_env.current_position, janus_env.full_x).values.reshape(1,-1)):0.03f} action {janus_env.last_action}&#39;)
    fig.add_trace(go.Scatter(x=[current_y[0]], y=[current_y[1]], name=&#39;point initial obs &#39;+str(janus_env.idx)))


    for i in range(100):
        action, _ = model.predict(janus_env.current_position)
    #     print(f&#39;action {action}&#39;)
        obs, rewards, done, info = janus_env.step(action)
    #     janus_env.render()
        if done:
            print(f&#39;done within {i+1} iterations&#39;)
            break
    new_y = janus_env.ml_model.predict(janus_env.convert_to_real_obs(janus_env.current_position,
                                         janus_env.full_x).values.reshape(1,-1)).reshape(-1)

    print(f&#39;final results:  {new_y} reward {janus_env.reward(janus_env.convert_to_real_obs(janus_env.current_position, janus_env.full_x).values.reshape(1,-1)):0.03f} action {janus_env.last_action}&#39;)
    fig.add_trace(go.Scatter(x=[new_y[0]], y=[new_y[1]], name=&#39;point final obs &#39;+str(janus_env.idx)))</code></pre>
</details>
</dd>
<dt id="janus_rl.train_RL_model"><code class="name flex">
<span>def <span class="ident">train_RL_model</span></span>(<span>exp_number, idx, reward_name)</span>
</code></dt>
<dd>
<div class="desc"><p>train a TD3 model with OrnsteinUhlenbeckActionNoise
10000 timesteps
starting with observation $idx$
and log it on tensorboard within entry: EXP$exp_number$ - IDX$idx - $reward_name</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_RL_model(exp_number, idx, reward_name):
    &#39;&#39;&#39;
    train a TD3 model with OrnsteinUhlenbeckActionNoise
    10000 timesteps
    starting with observation $idx$
    and log it on tensorboard within entry: EXP$exp_number$ - IDX$idx - $reward_name
    &#39;&#39;&#39;
    janus = Janus(idx, reward_name)
    check_env(janus)

    model_name = &#34;EXP {} - IDX {} - {}&#34;.format(exp_number, idx, reward_name)
    n_actions = janus.action_space.shape[-1]
    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))
    action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))

    model_janus_td3 = TD3(&#34;MlpPolicy&#34;, janus, action_noise=action_noise, verbose=2,tensorboard_log=&#34;./tensorboard/&#34;)
    model_janus_td3.learn(total_timesteps=10000, log_interval=4, tb_log_name=model_name)
    model_janus_td3.save(&#34;./data/&#34;+model_name)</code></pre>
</details>
</dd>
<dt id="janus_rl.visualise_avant_apres"><code class="name flex">
<span>def <span class="ident">visualise_avant_apres</span></span>(<span>exp_number, idx, reward_name)</span>
</code></dt>
<dd>
<div class="desc"><p>create janus env(idx, reward_name)
load rl model()
plot initial action and prescripted one after</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualise_avant_apres(exp_number, idx, reward_name):
    &#39;&#39;&#39;
    create janus env(idx, reward_name)
    load rl model()
    plot initial action and prescripted one after
    &#39;&#39;&#39;
    fig = go.Figure()

    janus = Janus(idx, reward_name)
    check_env(janus)
    janus.reset()
    RL_model = load_RL_model(exp_number, idx, reward_name)

    fig.update_xaxes(title_text=&#39;target_0&#39;, range=[inf_limx-3, sup_limx+3])
    fig.update_yaxes(title_text=&#39;target_5&#39;, range=[inf_limy-3, sup_limy+3])

    fig.add_trace(go.Scatter(x=[inf_x,inf_x,sup_x,sup_x, inf_x, None, vav_x], y=[inf_y,sup_y,sup_y, inf_y, inf_y, None, vav_y], fill=&#34;toself&#34;, name=&#39;TI, TS, VAV&#39;))
    positionne_point_idx(janus, RL_model, fig)
    fig.update_layout({&#39;showlegend&#39;: True, &#39;title&#39;:&#39;Position de la qualité, avant, après&#39;})
    fig.show()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="janus_rl.Janus"><code class="flex name class">
<span>class <span class="ident">Janus</span></span>
<span>(</span><span>idx=47, reward_function='clown_hat')</span>
</code></dt>
<dd>
<div class="desc"><p>The main OpenAI Gym class. It encapsulates an environment with
arbitrary behind-the-scenes dynamics. An environment can be
partially or fully observed.</p>
<p>The main API methods that users of this class need to know are:</p>
<pre><code>step
reset
render
close
seed
</code></pre>
<p>And set the following attributes:</p>
<pre><code>action_space: The Space object corresponding to valid actions
observation_space: The Space object corresponding to valid observations
reward_range: A tuple corresponding to the min and max possible rewards
</code></pre>
<p>Note: a default reward range set to [-inf,+inf] already exists. Set it if you want a narrower range.</p>
<p>The methods are accessed publicly as "step", "reset", etc&hellip;</p>
<p>idx: index of observation file (full_x 13000 dropouts), default = 47
reward_function: string among ['clown_hat', 'archery', 'smart_archery'], default 'clown_hat'</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Janus(gym.Env):
    metadata = {&#39;render.modes&#39;: [&#39;human&#39;]}
    template_filename = &#39;data/dataset-S_public/public/dataset_S-{}.csv&#39;

    def __init__(self, idx=47, reward_function=&#39;clown_hat&#39;):
        &#39;&#39;&#39;
        idx: index of observation file (full_x 13000 dropouts), default = 47
        reward_function: string among [&#39;clown_hat&#39;, &#39;archery&#39;, &#39;smart_archery&#39;], default &#39;clown_hat&#39;
        &#39;&#39;&#39;
        super(Janus, self).__init__()
        #actions: move on the grid, by continuous value in -1,1
        #0,0 no move
        #based on 94 controlable parameters
        #&#34;We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) &#34;, we will multiply effect by 2
#         self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(94, ))
        #we focus on the 1 most influencal action
        nbr_actions = 4
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(nbr_actions, ))


        # all the observation_space
        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(228, ))

        file1 = pd.read_csv(self.template_filename.format(&#39;file1&#39;), index_col=0)
        file2 = pd.read_csv(self.template_filename.format(&#39;file2&#39;), index_col=0)
        file3 = pd.read_csv(self.template_filename.format(&#39;file3&#39;), index_col=0)
        vav = pd.read_csv(self.template_filename.format(&#39;VAV&#39;), index_col=0)
        self.ti = pd.read_csv(self.template_filename.format(&#39;TI&#39;), index_col=0)
        self.ts = pd.read_csv(self.template_filename.format(&#39;TS&#39;), index_col=0)
        x_df = file1.copy()
        x_df = x_df.loc[:, (x_df != 0).any(axis=0)]  ## remove static columns
        x_df = x_df.fillna(x_df.mean())  ## replace nan with mean

        self.y_df = file2.copy()
        self.y_df.dropna(how=&#39;all&#39;, axis=1,
                    inplace=True)  ## drop full tempty columns
        self.y_df = self.y_df.fillna(self.y_df.mean())

        self.vav_df = vav.copy()
        # Dropping few columns
        for dataset in [self.y_df, self.vav_df, self.ti, self.ts]:
            dataset.drop([&#39;target_1&#39;, &#39;target_2&#39;, &#39;target_3&#39;, &#39;target_4&#39;],
                  axis=1,
                  inplace=True)  #to simplify with a 2-dimension target space

        print(&#39;features shape: {}, \ntargets shape: {}&#39;.format(
            x_df.shape, self.y_df.shape))

        x_train, x_test, y_train, y_test = train_test_split(x_df,
                                                            self.y_df,
                                                            test_size=0.1,
                                                            random_state=14)
        print(&#39;\nLength of train is {}, test is {}&#39;.format(
            len(x_train), len(x_test)))
        ## Random forest
        filename = &#39;data/models/janus_RF.pkl&#39;  # janus_LinearReg, janus_RF

        # pickle.dump(ml_model, open(filename, &#39;wb&#39;))

        # # load the model from disk
        self.ml_model = pickle.load(open(filename, &#39;rb&#39;))
        print(f&#39;R squared: {self.ml_model.score(x_test, y_test.values):0.04f}&#39;)

        self.full_x = file3.copy()[x_df.columns]
        self.full_x = self.full_x.fillna(x_df.mean())

        self.partial_x = x_train.copy()

        inferred_y = pd.DataFrame(self.ml_model.predict(self.full_x),
                                  columns=self.y_df.columns)

        # list of [min, max, step, range] for each var
        scale = 100
        decimals = 3

        self.list_important_actions = np.argsort(self.ml_model.feature_importances_[:94])[::-1][:nbr_actions]

        ## get limits for Rewards
        self.output_steps = [round((self.y_df[i].max() - self.y_df[i].min())/scale, decimals) \
                        for i in self.y_df.columns]
        print(&#39;Output steps: &#39;, self.output_steps)

        self.idx=idx

        assert reward_function in [&#39;clown_hat&#39;, &#39;archery&#39;, &#39;smart_archery&#39;], reward_function
        self.reward_function = reward_function
        print(f&#39;Active reward function {self.reward_function}&#39;)

    def reset(self):
#         self.current_position = self.revert_to_obs_space(
#             self.full_x.sample().values.reshape(-1), self.full_x)
#         random.seed(13)
#         idx = random.randint(0,len(janus_env.partial_x)-1)
        self.current_position = self.revert_to_obs_space(
            self.full_x.iloc[self.idx].values.reshape(-1), self.full_x)


        # to fix The observation returned by the `reset()` method does not match the given observation space
        # maybe won&#39;t happen on linux
        # on windows looks like float64 is the defautl for pandas -&gt; numpy and gym expects float32 (contains tries to cast to dtype(float32))
        self.current_position = self.current_position.astype(&#39;float32&#39;)

        self.last_action = self.current_position[self.list_important_actions]
        self.last_effect = False
        self.global_reward = 0
        self.episode_length = 0
        #print(f&#39;reset at position {self.current_position[:10]}...&#39;)
        return self.current_position

    def step(self, action):
#         self.current_position[0:len(action)] = action
        for index, act in enumerate(self.list_important_actions):
            self.current_position[act]=action[index]
        self.last_action = action
        self.episode_length += 1
        done = False

        reward = self.reward(
            self.convert_to_real_obs(self.current_position,
                                     self.full_x).values.reshape(1,-1))
        if (reward &gt;= -0.1*self.y_df.shape[1]):
            done = True

#         print(f&#39;target reached {done} reward {reward:0.03f} n° step {self.episode_length} action {self.last_action} done threeshold {-0.1*self.y_df.shape[1]:0.03f}&#39;)

        if done:
            reward += 100

        if self.episode_length&gt;100:
            print(&#39;episode too long -&gt; reset&#39;)
            done = True

#         if (max(abs(action))==1):
#             # if on border, we kill episode
#             print(&#39;border reached -&gt; done -&gt; reset&#39;)
#             reward -= 50
#             done = True


        self.global_reward += reward
#         print(f&#39;  reward {reward:0.03f} global reward {self.global_reward:0.03f} done {done} action {action} step {self.episode_length}&#39;)
        return self.current_position, reward, done, {}

    def render(self):
        print(
            f&#39;position {self.current_position[:10]}, action {self.last_action[:5]}, effect {self.last_effect}, done {done}, global_reward {self.global_reward:0.03f}&#39;
        )

    def convert_to_real_obs(self, observation, observation_dataset):
        &#39;&#39;&#39;
        to convert an observation from observation space ([-1, 1],325) to  real world
        -1 matches with min() of each column
        1 matches with max() of each column

        observation: instance of observation_space
        observation_dataset: the full real dataset (obfuscated in that case)
        &#39;&#39;&#39;
        return (observation + np.ones(self.observation_space.shape)) / 2 * (
            observation_dataset.max() -
            observation_dataset.min()) + observation_dataset.min()

    def revert_to_obs_space(self, real_observation, observation_dataset):
        &#39;&#39;&#39;
        to revert an observation sample (from real world) to observation space
        min() of each column will match with -1
        max() of each column will match with +1

        real_observation: instance of real_world
        observation_dataset: the full real dataset (obfuscated in that case)
        &#39;&#39;&#39;
        return np.nan_to_num(
            2 * (real_observation - observation_dataset.min()) /
            (observation_dataset.max() - observation_dataset.min()) -
            np.ones(self.observation_space.shape)).reshape(-1)

    def reward(self, observation):
        &#39;&#39;&#39;reward
        observation is from real world not observation space
        map reward function according to
        &#39;&#39;&#39;

        reward_function = self.reward_clown_hat
        if (self.reward_function == &#39;archery&#39;):
            reward_function = self.reward_archery
        if (self.reward_function == &#39;smart_archery&#39;):
            reward_function = self.reward_smart_archery

        new_y = self.ml_model.predict(observation).reshape(-1)
#         return self.continuous_reward_clown_hat(new_y)
        return reward_function(new_y)

    def discrete_reward(self, new_y):
        &#39;&#39;&#39; Discrete reward &#39;&#39;&#39;

        new_val = [
            sqrt((self.vav_df.iloc[:, i].values[0] - new_y[i])**2)
            for i in range(len(new_y))
        ]
        k = 10
        k1 = 1
        if new_val[0] &lt; k * self.output_steps[0] and new_val[
                1] &lt; k * self.output_steps[1]:
            reward = 1  #dans les 10% d&#39;amplitude max autour de la vav
            if new_val[0] &lt; k1 * self.output_steps[0] and new_val[
                    1] &lt; k1 * self.output_steps[1]:
                reward = 10  #dans les 1% d&#39;amplitude max autour de la vav
                on_target = True
#                 print(&#39;On Target : &#39;, new_y)
        else:
            reward = -1
        return reward



    def reward_clown_hat(self, new_y):
        &#39;&#39;&#39; clown_hat reward &#39;&#39;&#39;
        final_reward = 0

        for i in range(len(new_y)):
            reward = -9
            if ( self.ti.iloc[:,i].values[0] &lt;=  new_y[i] &lt;= self.ts.iloc[:,i].values[0]):
                if ( new_y[i] &gt;= self.vav_df.iloc[:,i].values[0] ):
                    reward = 1-(new_y[i]-self.vav_df.iloc[:,i].values[0])/(self.ts.iloc[:,i].values[0]-self.vav_df.iloc[:,i].values[0])
                else:
                    reward = 1-(self.vav_df.iloc[:,i].values[0]-new_y[i])/(self.vav_df.iloc[:,i].values[0]-self.ti.iloc[:,i].values[0])
            reward += -1
            final_reward+=reward
    #         print(f&#39;reward {reward} final_reward {final_reward} i {i}&#39;)

        if (final_reward&gt;0.7*len(new_y)):
            on_target = True
    #         print(&#39;On Target : &#39;, new_y)

        return final_reward

    def reward_archery(self, new_y):
        &#39;&#39;&#39; archery reward
        drawback is that you need progress on all targets to get reward improvment
        &#39;&#39;&#39;
        final_reward = 0

        ti_target_0 = self.ti.iloc[:,0].values[0]
        ts_target_0 = self.ts.iloc[:,0].values[0]
        ti_target_5 = self.ti.iloc[:,1].values[0]
        ts_target_5 = self.ts.iloc[:,1].values[0]
        x, y = new_y[0], new_y[1]
        if ( (ti_target_0*0.10 &lt;= x &lt;= ts_target_0*0.10) &amp; ( ti_target_5*0.10 &lt;= y &lt;= ts_target_5*0.10 )):
            reward = 0
        else:
            if ( (ti_target_0*0.50 &lt;= x &lt;= ts_target_0*0.50) &amp; ( ti_target_5*0.50 &lt;= y &lt;= ts_target_5*0.50 )):
                reward = -2
            else:
                if ( (ti_target_0 &lt;= x &lt;= ts_target_0) &amp; ( ti_target_5 &lt;= y &lt;= ts_target_5 )):
                    reward = -5
                else:
                    reward = -20
        final_reward = reward
        return final_reward

    def reward_smart_archery(self, new_y):
        &#39;&#39;&#39; smart archery reward &#39;&#39;&#39;
        ti_target_0 = self.ti.iloc[:,0].values[0]
        ts_target_0 = self.ts.iloc[:,0].values[0]
        ti_target_5 = self.ti.iloc[:,1].values[0]
        ts_target_5 = self.ts.iloc[:,1].values[0]
        x, y = new_y[0], new_y[1]

        reward_x = 0
        if (x &lt;= ti_target_0 or x &gt;= ts_target_0): reward_x = -10
        if ( ti_target_0 &lt;= x &lt;= 0.5*ti_target_0  or 0.5*ts_target_0 &lt;= x &lt;= ts_target_0  ): reward_x = -3
        if ( 0.5*ti_target_0 &lt;= x &lt;= 0.1*ti_target_0  or 0.1*ts_target_0 &lt;= x &lt;= 0.5*ts_target_0  ): reward_x = -1
        reward_y = 0
        if (y &lt;= ti_target_5 or y &gt;= ts_target_5): reward_y = -10
        if ( ti_target_5 &lt;= y &lt;= 0.5*ti_target_5  or 0.5*ts_target_5 &lt;= y &lt;= ts_target_5  ): reward_y = -3
        if ( 0.5*ti_target_5 &lt;= y &lt;= 0.1*ti_target_5  or 0.1*ts_target_5 &lt;= y &lt;= 0.5*ts_target_5  ): reward_y = -1

        final_reward = reward_x + reward_y
        return final_reward</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>gym.core.Env</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="janus_rl.Janus.metadata"><code class="name">var <span class="ident">metadata</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="janus_rl.Janus.template_filename"><code class="name">var <span class="ident">template_filename</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="janus_rl.Janus.convert_to_real_obs"><code class="name flex">
<span>def <span class="ident">convert_to_real_obs</span></span>(<span>self, observation, observation_dataset)</span>
</code></dt>
<dd>
<div class="desc"><p>to convert an observation from observation space ([-1, 1],325) to
real world
-1 matches with min() of each column
1 matches with max() of each column</p>
<p>observation: instance of observation_space
observation_dataset: the full real dataset (obfuscated in that case)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert_to_real_obs(self, observation, observation_dataset):
    &#39;&#39;&#39;
    to convert an observation from observation space ([-1, 1],325) to  real world
    -1 matches with min() of each column
    1 matches with max() of each column

    observation: instance of observation_space
    observation_dataset: the full real dataset (obfuscated in that case)
    &#39;&#39;&#39;
    return (observation + np.ones(self.observation_space.shape)) / 2 * (
        observation_dataset.max() -
        observation_dataset.min()) + observation_dataset.min()</code></pre>
</details>
</dd>
<dt id="janus_rl.Janus.discrete_reward"><code class="name flex">
<span>def <span class="ident">discrete_reward</span></span>(<span>self, new_y)</span>
</code></dt>
<dd>
<div class="desc"><p>Discrete reward</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    def discrete_reward(self, new_y):
        &#39;&#39;&#39; Discrete reward &#39;&#39;&#39;

        new_val = [
            sqrt((self.vav_df.iloc[:, i].values[0] - new_y[i])**2)
            for i in range(len(new_y))
        ]
        k = 10
        k1 = 1
        if new_val[0] &lt; k * self.output_steps[0] and new_val[
                1] &lt; k * self.output_steps[1]:
            reward = 1  #dans les 10% d&#39;amplitude max autour de la vav
            if new_val[0] &lt; k1 * self.output_steps[0] and new_val[
                    1] &lt; k1 * self.output_steps[1]:
                reward = 10  #dans les 1% d&#39;amplitude max autour de la vav
                on_target = True
#                 print(&#39;On Target : &#39;, new_y)
        else:
            reward = -1
        return reward</code></pre>
</details>
</dd>
<dt id="janus_rl.Janus.render"><code class="name flex">
<span>def <span class="ident">render</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Renders the environment.</p>
<p>The set of supported modes varies per environment. (And some
environments do not support rendering at all.) By convention,
if mode is:</p>
<ul>
<li>human: render to the current display or terminal and
return nothing. Usually for human consumption.</li>
<li>rgb_array: Return an numpy.ndarray with shape (x, y, 3),
representing RGB values for an x-by-y pixel image, suitable
for turning into a video.</li>
<li>ansi: Return a string (str) or StringIO.StringIO containing a
terminal-style text representation. The text can include newlines
and ANSI escape sequences (e.g. for colors).</li>
</ul>
<h2 id="note">Note</h2>
<p>Make sure that your class's metadata 'render.modes' key includes
the list of supported modes. It's recommended to call super()
in implementations to use the functionality of this method.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>the mode to render with</dd>
</dl>
<p>Example:</p>
<p>class MyEnv(Env):
metadata = {'render.modes': ['human', 'rgb_array']}</p>
<pre><code>def render(self, mode='human'):
    if mode == 'rgb_array':
        return np.array(...) # return RGB frame suitable for video
    elif mode == 'human':
        ... # pop up a window and render
    else:
        super(MyEnv, self).render(mode=mode) # just raise an exception
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render(self):
    print(
        f&#39;position {self.current_position[:10]}, action {self.last_action[:5]}, effect {self.last_effect}, done {done}, global_reward {self.global_reward:0.03f}&#39;
    )</code></pre>
</details>
</dd>
<dt id="janus_rl.Janus.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the environment to an initial state and returns an initial
observation.</p>
<p>Note that this function should not reset the environment's random
number generator(s); random variables in the environment's state should
be sampled independently between multiple calls to <code>reset()</code>. In other
words, each call of <code>reset()</code> should yield an environment suitable for
a new episode, independent of previous episodes.</p>
<h2 id="returns">Returns</h2>
<p>observation (object): the initial observation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    def reset(self):
#         self.current_position = self.revert_to_obs_space(
#             self.full_x.sample().values.reshape(-1), self.full_x)
#         random.seed(13)
#         idx = random.randint(0,len(janus_env.partial_x)-1)
        self.current_position = self.revert_to_obs_space(
            self.full_x.iloc[self.idx].values.reshape(-1), self.full_x)


        # to fix The observation returned by the `reset()` method does not match the given observation space
        # maybe won&#39;t happen on linux
        # on windows looks like float64 is the defautl for pandas -&gt; numpy and gym expects float32 (contains tries to cast to dtype(float32))
        self.current_position = self.current_position.astype(&#39;float32&#39;)

        self.last_action = self.current_position[self.list_important_actions]
        self.last_effect = False
        self.global_reward = 0
        self.episode_length = 0
        #print(f&#39;reset at position {self.current_position[:10]}...&#39;)
        return self.current_position</code></pre>
</details>
</dd>
<dt id="janus_rl.Janus.revert_to_obs_space"><code class="name flex">
<span>def <span class="ident">revert_to_obs_space</span></span>(<span>self, real_observation, observation_dataset)</span>
</code></dt>
<dd>
<div class="desc"><p>to revert an observation sample (from real world) to observation space
min() of each column will match with -1
max() of each column will match with +1</p>
<p>real_observation: instance of real_world
observation_dataset: the full real dataset (obfuscated in that case)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def revert_to_obs_space(self, real_observation, observation_dataset):
    &#39;&#39;&#39;
    to revert an observation sample (from real world) to observation space
    min() of each column will match with -1
    max() of each column will match with +1

    real_observation: instance of real_world
    observation_dataset: the full real dataset (obfuscated in that case)
    &#39;&#39;&#39;
    return np.nan_to_num(
        2 * (real_observation - observation_dataset.min()) /
        (observation_dataset.max() - observation_dataset.min()) -
        np.ones(self.observation_space.shape)).reshape(-1)</code></pre>
</details>
</dd>
<dt id="janus_rl.Janus.reward"><code class="name flex">
<span>def <span class="ident">reward</span></span>(<span>self, observation)</span>
</code></dt>
<dd>
<div class="desc"><p>reward
observation is from real world not observation space
map reward function according to</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    def reward(self, observation):
        &#39;&#39;&#39;reward
        observation is from real world not observation space
        map reward function according to
        &#39;&#39;&#39;

        reward_function = self.reward_clown_hat
        if (self.reward_function == &#39;archery&#39;):
            reward_function = self.reward_archery
        if (self.reward_function == &#39;smart_archery&#39;):
            reward_function = self.reward_smart_archery

        new_y = self.ml_model.predict(observation).reshape(-1)
#         return self.continuous_reward_clown_hat(new_y)
        return reward_function(new_y)</code></pre>
</details>
</dd>
<dt id="janus_rl.Janus.reward_archery"><code class="name flex">
<span>def <span class="ident">reward_archery</span></span>(<span>self, new_y)</span>
</code></dt>
<dd>
<div class="desc"><p>archery reward
drawback is that you need progress on all targets to get reward improvment</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reward_archery(self, new_y):
    &#39;&#39;&#39; archery reward
    drawback is that you need progress on all targets to get reward improvment
    &#39;&#39;&#39;
    final_reward = 0

    ti_target_0 = self.ti.iloc[:,0].values[0]
    ts_target_0 = self.ts.iloc[:,0].values[0]
    ti_target_5 = self.ti.iloc[:,1].values[0]
    ts_target_5 = self.ts.iloc[:,1].values[0]
    x, y = new_y[0], new_y[1]
    if ( (ti_target_0*0.10 &lt;= x &lt;= ts_target_0*0.10) &amp; ( ti_target_5*0.10 &lt;= y &lt;= ts_target_5*0.10 )):
        reward = 0
    else:
        if ( (ti_target_0*0.50 &lt;= x &lt;= ts_target_0*0.50) &amp; ( ti_target_5*0.50 &lt;= y &lt;= ts_target_5*0.50 )):
            reward = -2
        else:
            if ( (ti_target_0 &lt;= x &lt;= ts_target_0) &amp; ( ti_target_5 &lt;= y &lt;= ts_target_5 )):
                reward = -5
            else:
                reward = -20
    final_reward = reward
    return final_reward</code></pre>
</details>
</dd>
<dt id="janus_rl.Janus.reward_clown_hat"><code class="name flex">
<span>def <span class="ident">reward_clown_hat</span></span>(<span>self, new_y)</span>
</code></dt>
<dd>
<div class="desc"><p>clown_hat reward</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reward_clown_hat(self, new_y):
    &#39;&#39;&#39; clown_hat reward &#39;&#39;&#39;
    final_reward = 0

    for i in range(len(new_y)):
        reward = -9
        if ( self.ti.iloc[:,i].values[0] &lt;=  new_y[i] &lt;= self.ts.iloc[:,i].values[0]):
            if ( new_y[i] &gt;= self.vav_df.iloc[:,i].values[0] ):
                reward = 1-(new_y[i]-self.vav_df.iloc[:,i].values[0])/(self.ts.iloc[:,i].values[0]-self.vav_df.iloc[:,i].values[0])
            else:
                reward = 1-(self.vav_df.iloc[:,i].values[0]-new_y[i])/(self.vav_df.iloc[:,i].values[0]-self.ti.iloc[:,i].values[0])
        reward += -1
        final_reward+=reward
#         print(f&#39;reward {reward} final_reward {final_reward} i {i}&#39;)

    if (final_reward&gt;0.7*len(new_y)):
        on_target = True
#         print(&#39;On Target : &#39;, new_y)

    return final_reward</code></pre>
</details>
</dd>
<dt id="janus_rl.Janus.reward_smart_archery"><code class="name flex">
<span>def <span class="ident">reward_smart_archery</span></span>(<span>self, new_y)</span>
</code></dt>
<dd>
<div class="desc"><p>smart archery reward</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reward_smart_archery(self, new_y):
    &#39;&#39;&#39; smart archery reward &#39;&#39;&#39;
    ti_target_0 = self.ti.iloc[:,0].values[0]
    ts_target_0 = self.ts.iloc[:,0].values[0]
    ti_target_5 = self.ti.iloc[:,1].values[0]
    ts_target_5 = self.ts.iloc[:,1].values[0]
    x, y = new_y[0], new_y[1]

    reward_x = 0
    if (x &lt;= ti_target_0 or x &gt;= ts_target_0): reward_x = -10
    if ( ti_target_0 &lt;= x &lt;= 0.5*ti_target_0  or 0.5*ts_target_0 &lt;= x &lt;= ts_target_0  ): reward_x = -3
    if ( 0.5*ti_target_0 &lt;= x &lt;= 0.1*ti_target_0  or 0.1*ts_target_0 &lt;= x &lt;= 0.5*ts_target_0  ): reward_x = -1
    reward_y = 0
    if (y &lt;= ti_target_5 or y &gt;= ts_target_5): reward_y = -10
    if ( ti_target_5 &lt;= y &lt;= 0.5*ti_target_5  or 0.5*ts_target_5 &lt;= y &lt;= ts_target_5  ): reward_y = -3
    if ( 0.5*ti_target_5 &lt;= y &lt;= 0.1*ti_target_5  or 0.1*ts_target_5 &lt;= y &lt;= 0.5*ts_target_5  ): reward_y = -1

    final_reward = reward_x + reward_y
    return final_reward</code></pre>
</details>
</dd>
<dt id="janus_rl.Janus.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, action)</span>
</code></dt>
<dd>
<div class="desc"><p>Run one timestep of the environment's dynamics. When end of
episode is reached, you are responsible for calling <code>reset()</code>
to reset this environment's state.</p>
<p>Accepts an action and returns a tuple (observation, reward, done, info).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>action</code></strong> :&ensp;<code>object</code></dt>
<dd>an action provided by the agent</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>observation (object): agent's observation of the current environment
reward (float) : amount of reward returned after previous action
done (bool): whether the episode has ended, in which case further step() calls will return undefined results
info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    def step(self, action):
#         self.current_position[0:len(action)] = action
        for index, act in enumerate(self.list_important_actions):
            self.current_position[act]=action[index]
        self.last_action = action
        self.episode_length += 1
        done = False

        reward = self.reward(
            self.convert_to_real_obs(self.current_position,
                                     self.full_x).values.reshape(1,-1))
        if (reward &gt;= -0.1*self.y_df.shape[1]):
            done = True

#         print(f&#39;target reached {done} reward {reward:0.03f} n° step {self.episode_length} action {self.last_action} done threeshold {-0.1*self.y_df.shape[1]:0.03f}&#39;)

        if done:
            reward += 100

        if self.episode_length&gt;100:
            print(&#39;episode too long -&gt; reset&#39;)
            done = True

#         if (max(abs(action))==1):
#             # if on border, we kill episode
#             print(&#39;border reached -&gt; done -&gt; reset&#39;)
#             reward -= 50
#             done = True


        self.global_reward += reward
#         print(f&#39;  reward {reward:0.03f} global reward {self.global_reward:0.03f} done {done} action {action} step {self.episode_length}&#39;)
        return self.current_position, reward, done, {}</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="janus_rl.calculate_reward_dropouts" href="#janus_rl.calculate_reward_dropouts">calculate_reward_dropouts</a></code></li>
<li><code><a title="janus_rl.calculate_reward_outputs" href="#janus_rl.calculate_reward_outputs">calculate_reward_outputs</a></code></li>
<li><code><a title="janus_rl.load_RL_model" href="#janus_rl.load_RL_model">load_RL_model</a></code></li>
<li><code><a title="janus_rl.plot_reward" href="#janus_rl.plot_reward">plot_reward</a></code></li>
<li><code><a title="janus_rl.plot_reward_of_dropouts" href="#janus_rl.plot_reward_of_dropouts">plot_reward_of_dropouts</a></code></li>
<li><code><a title="janus_rl.positionne_point_idx" href="#janus_rl.positionne_point_idx">positionne_point_idx</a></code></li>
<li><code><a title="janus_rl.train_RL_model" href="#janus_rl.train_RL_model">train_RL_model</a></code></li>
<li><code><a title="janus_rl.visualise_avant_apres" href="#janus_rl.visualise_avant_apres">visualise_avant_apres</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="janus_rl.Janus" href="#janus_rl.Janus">Janus</a></code></h4>
<ul class="">
<li><code><a title="janus_rl.Janus.convert_to_real_obs" href="#janus_rl.Janus.convert_to_real_obs">convert_to_real_obs</a></code></li>
<li><code><a title="janus_rl.Janus.discrete_reward" href="#janus_rl.Janus.discrete_reward">discrete_reward</a></code></li>
<li><code><a title="janus_rl.Janus.metadata" href="#janus_rl.Janus.metadata">metadata</a></code></li>
<li><code><a title="janus_rl.Janus.render" href="#janus_rl.Janus.render">render</a></code></li>
<li><code><a title="janus_rl.Janus.reset" href="#janus_rl.Janus.reset">reset</a></code></li>
<li><code><a title="janus_rl.Janus.revert_to_obs_space" href="#janus_rl.Janus.revert_to_obs_space">revert_to_obs_space</a></code></li>
<li><code><a title="janus_rl.Janus.reward" href="#janus_rl.Janus.reward">reward</a></code></li>
<li><code><a title="janus_rl.Janus.reward_archery" href="#janus_rl.Janus.reward_archery">reward_archery</a></code></li>
<li><code><a title="janus_rl.Janus.reward_clown_hat" href="#janus_rl.Janus.reward_clown_hat">reward_clown_hat</a></code></li>
<li><code><a title="janus_rl.Janus.reward_smart_archery" href="#janus_rl.Janus.reward_smart_archery">reward_smart_archery</a></code></li>
<li><code><a title="janus_rl.Janus.step" href="#janus_rl.Janus.step">step</a></code></li>
<li><code><a title="janus_rl.Janus.template_filename" href="#janus_rl.Janus.template_filename">template_filename</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>