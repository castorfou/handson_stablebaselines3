{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyyN-2qyK_T2"
   },
   "source": [
    "# Stable Baselines3 Hands-on Session - RLVS\n",
    "\n",
    "Github repo: https://github.com/araffin/rl-handson-rlvs21\n",
    "\n",
    "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
    "\n",
    "Documentation: https://stable-baselines3.readthedocs.io/en/master/\n",
    "\n",
    "SB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
    "\n",
    "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "\n",
    "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n",
    "\n",
    "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, you will learn the basics for using stable baselines3 library: how to create a RL model, train it and evaluate it. Because all algorithms share the same interface, we will see how simple it is to switch from one algorithm to another.\n",
    "You will also learn how to define a gym wrapper and callback to customise the training.\n",
    "We will finish this session by trying out multiprocessing and have a hyperparameter tuning challenge.\n",
    "\n",
    "\n",
    "## Install Dependencies and Stable Baselines3 Using Pip\n",
    "\n",
    "List of full dependencies can be found in the [README](https://github.com/DLR-RM/stable-baselines3).\n",
    "\n",
    "\n",
    "```\n",
    "pip install stable-baselines3[extra]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gWskDE2c9WoN",
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hYdv2ygjLaFL",
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T08:52:33.125281Z",
     "start_time": "2021-04-09T08:52:17.782896Z"
    },
    "id": "oexj67yWN5_k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sb3-contrib\n",
      "  Downloading sb3_contrib-1.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: stable-baselines3[docs,tests]>=1.0 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from sb3-contrib) (1.0)\n",
      "Requirement already satisfied: cloudpickle in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.6.0)\n",
      "Requirement already satisfied: gym>=0.17 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.18.0)\n",
      "Requirement already satisfied: matplotlib in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.3.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.7.1)\n",
      "Requirement already satisfied: pandas in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.2.3)\n",
      "Requirement already satisfied: numpy in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.20.1)\n",
      "Collecting sphinx\n",
      "  Downloading Sphinx-3.5.3-py3-none-any.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sphinx-autobuild\n",
      "  Downloading sphinx_autobuild-2021.3.14-py3-none-any.whl (9.9 kB)\n",
      "Collecting sphinxcontrib.spelling\n",
      "  Downloading sphinxcontrib_spelling-7.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting sphinx-autodoc-typehints\n",
      "  Downloading sphinx_autodoc_typehints-1.11.1-py3-none-any.whl (8.7 kB)\n",
      "Collecting sphinx-rtd-theme\n",
      "  Downloading sphinx_rtd_theme-0.5.2-py2.py3-none-any.whl (9.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.1 MB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flake8>=3.8\n",
      "  Downloading flake8-3.9.0-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 2.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pytest-env\n",
      "  Downloading pytest-env-0.6.2.tar.gz (1.7 kB)\n",
      "Collecting pytest-xdist\n",
      "  Downloading pytest_xdist-2.2.1-py3-none-any.whl (37 kB)\n",
      "Collecting pytype\n",
      "  Downloading pytype-2021.4.1-cp37-cp37m-manylinux2014_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 27.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting black\n",
      "  Downloading black-20.8b1.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 20.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytest-cov\n",
      "  Downloading pytest_cov-2.11.1-py2.py3-none-any.whl (20 kB)\n",
      "Collecting isort>=5.0\n",
      "  Downloading isort-5.8.0-py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 32.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytest\n",
      "  Downloading pytest-6.2.3-py3-none-any.whl (280 kB)\n",
      "\u001b[K     |████████████████████████████████| 280 kB 23.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycodestyle<2.8.0,>=2.7.0\n",
      "  Downloading pycodestyle-2.7.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 898 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from flake8>=3.8->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.7.3)\n",
      "Collecting pyflakes<2.4.0,>=2.3.0\n",
      "  Downloading pyflakes-2.3.1-py2.py3-none-any.whl (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 9.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
      "  Using cached mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in /home/explore/.local/lib/python3.7/site-packages (from gym>=0.17->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (7.2.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from gym>=0.17->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.5.0)\n",
      "Requirement already satisfied: scipy in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from gym>=0.17->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.6.1)\n",
      "Requirement already satisfied: future in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.18.2)\n",
      "Requirement already satisfied: typing_extensions in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from torch>=1.4.0->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.7.4.3)\n",
      "Collecting pathspec<1,>=0.6\n",
      "  Downloading pathspec-0.8.1-py2.py3-none-any.whl (28 kB)\n",
      "Collecting click>=7.1.2\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting typed-ast>=1.4.0\n",
      "  Downloading typed_ast-1.4.2-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
      "\u001b[K     |████████████████████████████████| 743 kB 22.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting appdirs\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting toml>=0.10.1\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting mypy-extensions>=0.4.3\n",
      "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Collecting regex>=2020.1.8\n",
      "  Downloading regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl (720 kB)\n",
      "\u001b[K     |████████████████████████████████| 720 kB 23.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from importlib-metadata->flake8>=3.8->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.4.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from matplotlib->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from matplotlib->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from matplotlib->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from matplotlib->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.8.1)\n",
      "Requirement already satisfied: six in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from pandas->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2021.1)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.10.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 7.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from pytest->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (20.9)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from pytest->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (20.3.0)\n",
      "Collecting pluggy<1.0.0a1,>=0.12\n",
      "  Using cached pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting iniconfig\n",
      "  Using cached iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting coverage>=5.2.1\n",
      "  Downloading coverage-5.5-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n",
      "\u001b[K     |████████████████████████████████| 242 kB 105.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting execnet>=1.1\n",
      "  Downloading execnet-1.8.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting pytest-forked\n",
      "  Downloading pytest_forked-1.3.0-py2.py3-none-any.whl (4.7 kB)\n",
      "Collecting apipkg>=1.4\n",
      "  Downloading apipkg-1.5-py2.py3-none-any.whl (4.9 kB)\n",
      "Collecting ninja>=1.10.0.post2\n",
      "  Downloading ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 29.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=3.11 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from pytype->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (5.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting importlab>=0.6.1\n",
      "  Downloading importlab-0.6.1.tar.gz (26 kB)\n",
      "Collecting networkx>=2\n",
      "  Downloading networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 21.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator<5,>=4.3 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from networkx>=2->importlab>=0.6.1->pytype->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (4.4.2)\n",
      "Collecting sphinxcontrib-jsmath\n",
      "  Using cached sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied: requests>=2.5.0 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from sphinx->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.25.1)\n",
      "Collecting alabaster<0.8,>=0.7\n",
      "  Using cached alabaster-0.7.12-py2.py3-none-any.whl (14 kB)\n",
      "Collecting sphinxcontrib-qthelp\n",
      "  Using cached sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
      "Collecting docutils>=0.12\n",
      "  Downloading docutils-0.17-py2.py3-none-any.whl (575 kB)\n",
      "\u001b[K     |████████████████████████████████| 575 kB 17.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sphinxcontrib-applehelp\n",
      "  Using cached sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: Pygments>=2.0 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from sphinx->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.8.1)\n",
      "Requirement already satisfied: Jinja2>=2.3 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from sphinx->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.11.3)\n",
      "Collecting imagesize\n",
      "  Using cached imagesize-1.2.0-py2.py3-none-any.whl (4.8 kB)\n",
      "Collecting sphinxcontrib-htmlhelp\n",
      "  Using cached sphinxcontrib_htmlhelp-1.0.3-py2.py3-none-any.whl (96 kB)\n",
      "Collecting sphinxcontrib-devhelp\n",
      "  Using cached sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
      "Collecting snowballstemmer>=1.1\n",
      "  Downloading snowballstemmer-2.1.0-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 3.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from sphinx->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (52.0.0.post20210125)\n",
      "Collecting sphinxcontrib-serializinghtml\n",
      "  Using cached sphinxcontrib_serializinghtml-1.1.4-py2.py3-none-any.whl (89 kB)\n",
      "Collecting babel>=1.3\n",
      "  Downloading Babel-2.9.0-py2.py3-none-any.whl (8.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.8 MB 18.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from Jinja2>=2.3->sphinx->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.1.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from requests>=2.5.0->sphinx->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from requests>=2.5.0->sphinx->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from requests>=2.5.0->sphinx->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from requests>=2.5.0->sphinx->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2020.12.5)\n",
      "Collecting livereload\n",
      "  Downloading livereload-2.6.3.tar.gz (25 kB)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: tornado in /home/explore/miniconda3/envs/stablebaselines3/lib/python3.7/site-packages (from livereload->sphinx-autobuild->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (6.1)\n",
      "Collecting docutils>=0.12\n",
      "  Using cached docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
      "Collecting PyEnchant>=3.1.1\n",
      "  Downloading pyenchant-3.2.0-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 6.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: black, pytest-env, importlab, livereload\n",
      "  Building wheel for black (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for black: filename=black-20.8b1-py3-none-any.whl size=124184 sha256=ef46e9d9e781f64c969bd6da9483018de3a6e6a51a584fbdd162e17dceddb957\n",
      "  Stored in directory: /home/explore/.cache/pip/wheels/c5/85/79/f3af8daaf8037c0bf14beb3b7a1511a39b6e6902ca2aaf494e\n",
      "  Building wheel for pytest-env (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytest-env: filename=pytest_env-0.6.2-py3-none-any.whl size=2370 sha256=81be6778abb3a0ef568e0e0eb55c1dee63ff183325757a5bcbe2576f7b9a6de9\n",
      "  Stored in directory: /home/explore/.cache/pip/wheels/00/78/6f/8413b85149939cd491ef2c5d3c5422e49cb2f898c8402f81f7\n",
      "  Building wheel for importlab (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for importlab: filename=importlab-0.6.1-py2.py3-none-any.whl size=21288 sha256=fa6b23ce7dd660f31cc6f6ee190be53a62b7cda20707ba05c04d7eb8963d4f08\n",
      "  Stored in directory: /home/explore/.cache/pip/wheels/b0/fa/22/7dc08c0a692b64347b964b10bbacbee247e9d8a3d00a2c0945\n",
      "  Building wheel for livereload (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for livereload: filename=livereload-2.6.3-py2.py3-none-any.whl size=24713 sha256=225d6b93b23cf78cb7b8540422660fdd25d09c6e44fd82eaa1c6652d5b2b5ccc\n",
      "  Stored in directory: /home/explore/.cache/pip/wheels/d4/f2/03/55f37093eb8cb0c89d7efb206f792dba55cd5bd67b1c5b1ce1\n",
      "Successfully built black pytest-env importlab livereload\n",
      "Installing collected packages: toml, py, pluggy, iniconfig, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, snowballstemmer, pytest, networkx, imagesize, docutils, babel, apipkg, alabaster, typed-ast, sphinx, regex, pytest-forked, pyflakes, PyEnchant, pycodestyle, pathspec, ninja, mypy-extensions, mccabe, livereload, importlab, execnet, coverage, colorama, click, appdirs, sphinxcontrib.spelling, sphinx-rtd-theme, sphinx-autodoc-typehints, sphinx-autobuild, pytype, pytest-xdist, pytest-env, pytest-cov, isort, flake8, black, sb3-contrib\n",
      "Successfully installed PyEnchant-3.2.0 alabaster-0.7.12 apipkg-1.5 appdirs-1.4.4 babel-2.9.0 black-20.8b1 click-7.1.2 colorama-0.4.4 coverage-5.5 docutils-0.16 execnet-1.8.0 flake8-3.9.0 imagesize-1.2.0 importlab-0.6.1 iniconfig-1.1.1 isort-5.8.0 livereload-2.6.3 mccabe-0.6.1 mypy-extensions-0.4.3 networkx-2.5.1 ninja-1.10.0.post2 pathspec-0.8.1 pluggy-0.13.1 py-1.10.0 pycodestyle-2.7.0 pyflakes-2.3.1 pytest-6.2.3 pytest-cov-2.11.1 pytest-env-0.6.2 pytest-forked-1.3.0 pytest-xdist-2.2.1 pytype-2021.4.1 regex-2021.4.4 sb3-contrib-1.0 snowballstemmer-2.1.0 sphinx-3.5.3 sphinx-autobuild-2021.3.14 sphinx-autodoc-typehints-1.11.1 sphinx-rtd-theme-0.5.2 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-1.0.3 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 sphinxcontrib-serializinghtml-1.1.4 sphinxcontrib.spelling toml-0.10.2 typed-ast-1.4.2\n"
     ]
    }
   ],
   "source": [
    "# Optional: install SB3 contrib to have access to additional algorithms\n",
    "!pip install sb3-contrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-khNkrgcI6Z1"
   },
   "source": [
    "# Part I: Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzevZcgmJmhi"
   },
   "source": [
    "## First steps with the Gym interface\n",
    "\n",
    "An environment that follows the [gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html) is quite simple to use.\n",
    "It provides to this user mainly three methods:\n",
    "- `reset()` called at the beginning of an episode, it returns an observation\n",
    "- `step(action)` called to take an action with the environment, it returns the next observation, the immediate reward, whether the episode is over and additional information\n",
    "- (Optional) `render(method='human')` which allow to visualize the agent in action. Note that graphical interface does not work on google colab, so we cannot use it directly (we have to rely on `method='rbg_array'` to retrieve an image of the scene\n",
    "\n",
    "Under the hood, it also contains two useful properties:\n",
    "- `observation_space` which one of the gym spaces (`Discrete`, `Box`, ...) and describe the type and shape of the observation\n",
    "- `action_space` which is also a gym space object that describes the action space, so the type of action that can be taken\n",
    "\n",
    "The best way to learn about gym spaces is to look at the [source code](https://github.com/openai/gym/tree/master/gym/spaces), but you need to know at least the main ones:\n",
    "- `gym.spaces.Box`: A (possibly unbounded) box in $R^n$. Specifically, a Box represents the Cartesian product of n closed intervals. Each interval has the form of one of [a, b], (-oo, b], [a, oo), or (-oo, oo). Example: A 1D-Vector or an image observation can be described with the Box space.\n",
    "```python\n",
    "# Example for using image as input:\n",
    "observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
    "```                                       \n",
    "\n",
    "- `gym.spaces.Discrete`: A discrete space in $\\{ 0, 1, \\dots, n-1 \\}$\n",
    "  Example: if you have two actions (\"left\" and \"right\") you can represent your action space using `Discrete(2)`, the first action will be 0 and the second 1.\n",
    "\n",
    "\n",
    "\n",
    "[Documentation on custom env](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html)\n",
    "\n",
    "Below you can find an example of a custom environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T08:56:07.295531Z",
     "start_time": "2021-04-09T08:56:07.277858Z"
    },
    "id": "owrHOGhlMGLE"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, NamedTuple, Tuple, Union\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "GymObs = Union[Tuple, Dict, np.ndarray, int]\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  Minimal custom environment to demonstrate the Gym interface.\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(CustomEnv, self).__init__()\n",
    "    self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(14,))\n",
    "    self.action_space = gym.spaces.Box(low=-1, high=1, shape=(6,))\n",
    "\n",
    "  def reset(self) -> GymObs:\n",
    "    \"\"\"\n",
    "    Called at the beginning of an episode.\n",
    "    :return: the first observation of the episode\n",
    "    \"\"\"\n",
    "    return self.observation_space.sample()\n",
    "\n",
    "  def step(self, action: Union[int, np.ndarray]) -> Tuple[GymObs, float, bool, Dict]:\n",
    "    \"\"\"\n",
    "    Step into the environment.\n",
    "    :return: A tuple containing the new observation, the reward signal, \n",
    "      whether the episode is over and additional informations.\n",
    "    \"\"\"\n",
    "    obs = self.observation_space.sample()\n",
    "    reward = 1.0\n",
    "    done = False\n",
    "    info = {}\n",
    "    return obs, reward, done, info\n",
    "\n",
    "env = CustomEnv()\n",
    "# Check your custom environment\n",
    "# this will print warnings and throw errors if needed\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtY8FhliLsGm"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcX8hEcaUpR0"
   },
   "source": [
    "Stable-Baselines3 works on environments that follow the [gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html).\n",
    "You can find a list of available environment [here](https://gym.openai.com/envs/#classic_control).\n",
    "\n",
    "It is also recommended to check the [source code](https://github.com/openai/gym) to learn more about the observation and action space of each env, as gym does not have a proper documentation.\n",
    "Not all algorithms can work with all action spaces, you can find more in this [recap table](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T08:56:41.538684Z",
     "start_time": "2021-04-09T08:56:41.534417Z"
    },
    "id": "BIedd7Pz9sOs"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ae32CtgzTG3R"
   },
   "source": [
    "The first thing you need to import is the RL model, check the documentation to know what you can use on which problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T08:56:42.720658Z",
     "start_time": "2021-04-09T08:56:42.715002Z"
    },
    "id": "R7tKaBFrTR0a"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C, SAC, TD3, DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T08:56:43.377785Z",
     "start_time": "2021-04-09T08:56:43.368455Z"
    },
    "id": "EcsXmYRMON9W"
   },
   "outputs": [],
   "source": [
    "# Algorithms from the contrib repo\n",
    "# https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
    "from sb3_contrib import QRDQN, TQC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0_8OQbOTTNT"
   },
   "source": [
    "The next thing you need to import is the policy class that will be used to create the networks (for the policy/value functions).\n",
    "This step is optional as you can directly use strings in the constructor: \n",
    "\n",
    "```PPO(\"MlpPolicy\", env)``` instead of ```PPO(MlpPolicy, env)```\n",
    "\n",
    "Note that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommended option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T08:56:47.389879Z",
     "start_time": "2021-04-09T08:56:47.387299Z"
    },
    "id": "ROUJr675TT01"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.ppo.policies import MlpPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RapkYvTXL7Cd"
   },
   "source": [
    "## Create the Gym env and instantiate the agent\n",
    "\n",
    "For this example, we will use CartPole environment, a classic control problem.\n",
    "\n",
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
    "\n",
    "Cartpole environment: [https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)\n",
    "\n",
    "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n",
    "\n",
    "\n",
    "We chose the MlpPolicy because the observation of the CartPole task is a feature vector, not images.\n",
    "\n",
    "The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n",
    "\n",
    "Here we are using the [Proximal Policy Optimization](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) algorithm, which is an Actor-Critic method: it uses a value function to improve the policy gradient descent (by reducing the variance).\n",
    "\n",
    "It combines ideas from [A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html) (having multiple workers and using an entropy bonus for exploration) and [TRPO](https://stable-baselines.readthedocs.io/en/master/modules/trpo.html) (it uses a trust region to improve stability and avoid catastrophic drops in performance).\n",
    "\n",
    "PPO is an on-policy algorithm, which means that the trajectories used to update the networks must be collected using the latest policy.\n",
    "It is usually less sample efficient than off-policy alorithms like [DQN](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html) or [TD3](https://stable-baselines3.readthedocs.io/en/master/modules/td3.html), but is much faster regarding wall-clock time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T08:59:04.357957Z",
     "start_time": "2021-04-09T08:59:04.320571Z"
    },
    "id": "pUWGZp3i9wyf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Create the gym Env\n",
    "env = gym.make('CartPole-v1')\n",
    "# Create the RL agent\n",
    "model = PPO('MlpPolicy', env, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0K2YzpGDwQ4"
   },
   "source": [
    "### Using the model to predict actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T08:59:12.435597Z",
     "start_time": "2021-04-09T08:59:12.429633Z"
    },
    "id": "JRfb_f7uD-H9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:00:17.109133Z",
     "start_time": "2021-04-09T09:00:17.104406Z"
    },
    "id": "rF_xsDEADvv_"
   },
   "outputs": [],
   "source": [
    "# Retrieve first observation\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:00:22.803826Z",
     "start_time": "2021-04-09T09:00:22.787333Z"
    },
    "id": "ZgrRoM23EU9Z"
   },
   "outputs": [],
   "source": [
    "# Predict the action to take given the observation\n",
    "action, _ = model.predict(obs, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:00:29.709674Z",
     "start_time": "2021-04-09T09:00:29.706154Z"
    },
    "id": "B1wRSvPrEjUa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# We are using discrete actions, therefore `action` is an int\n",
    "assert env.action_space.contains(action)\n",
    "\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZtEtKqKEvFC"
   },
   "source": [
    "Step in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:01:13.848084Z",
     "start_time": "2021-04-09T09:01:13.842509Z"
    },
    "id": "BmJ-yv5MEllY"
   },
   "outputs": [],
   "source": [
    "obs, reward, done, infos = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:01:14.834457Z",
     "start_time": "2021-04-09T09:01:14.828219Z"
    },
    "id": "VTNOOlOdE1lH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_shape=(4,), reward=1.0, done? False\n"
     ]
    }
   ],
   "source": [
    "print(f\"obs_shape={obs.shape}, reward={reward}, done? {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:01:36.325119Z",
     "start_time": "2021-04-09T09:01:36.322372Z"
    },
    "id": "2Dd6hT873ImE"
   },
   "outputs": [],
   "source": [
    "# Reset the env at the end of an episode\n",
    "if done:\n",
    "  obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iSeXppUFNLg"
   },
   "source": [
    "### Exercise (10 minutes): write the function to evaluate the agent\n",
    "\n",
    "This function will be used to evaluate the performance of an RL agent.\n",
    "Thanks to Stable Baselines3 interface, it will work with any SB3 algorithms and any Gym environment.\n",
    "\n",
    "See docstring of the function for what is expected as input/output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:22:08.281381Z",
     "start_time": "2021-04-09T09:22:08.271941Z"
    },
    "id": "Xf7Ktha2FE3B"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: BaseAlgorithm,\n",
    "    env: gym.Env,\n",
    "    n_eval_episodes: int = 100,\n",
    "    deterministic: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate an RL agent for `n_eval_episodes`.\n",
    "\n",
    "    :param model: the RL Agent\n",
    "    :param env: the gym Environment\n",
    "    :param n_eval_episodes: number of episodes to evaluate it\n",
    "    :param deterministic: Whether to use deterministic or stochastic actions\n",
    "    :return: Mean reward for the last `n_eval_episodes`\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    # TODO: run `n_eval_episodes` episodes in the Gym env\n",
    "    # using the RL agent and keep track of the total reward\n",
    "    # collected for each episode.\n",
    "    # Finally, compute the mean and print it\n",
    "    rewards_list=[]\n",
    "    for i in range(n_eval_episodes):\n",
    "      obs = env.reset()\n",
    "      done=False\n",
    "      reward_sum=0\n",
    "      while(not done):\n",
    "        action, _states = model.predict(obs, deterministic=deterministic)\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        reward_sum+=rewards\n",
    "      rewards_list.append(reward_sum)\n",
    "    mean_episode_reward = np.sum(rewards_list)/n_eval_episodes\n",
    "    print(f\"mean_reward={mean_episode_reward}, number_episodes={n_eval_episodes}\")\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "    return mean_episode_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3SahgtZH2TX"
   },
   "source": [
    "Let's evaluate the un-trained agent, this should be a random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:22:09.095322Z",
     "start_time": "2021-04-09T09:22:09.068949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "model = PPO('MlpPolicy',  env, seed=1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:22:11.088477Z",
     "start_time": "2021-04-09T09:22:09.453904Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8XGmJK4oHUeT",
    "outputId": "f3b1fbf7-d6c3-470c-e6ae-8e61a3fe4be6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=21.57, number_episodes=100\n"
     ]
    }
   ],
   "source": [
    "# Random Agent, before training\n",
    "mean_reward_before_train = evaluate(model, env, n_eval_episodes=100, deterministic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjjPxrwkYJ2i"
   },
   "source": [
    "Stable-Baselines already provides you with that helper (the actual implementation is a little more advanced):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:23:19.535172Z",
     "start_time": "2021-04-09T09:23:19.532596Z"
    },
    "id": "8z6K9YImYJEx"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:23:20.259331Z",
     "start_time": "2021-04-09T09:23:20.254440Z"
    },
    "id": "3lHSorUyH8I8"
   },
   "outputs": [],
   "source": [
    "# The Monitor wrapper allows to keep track of the training reward and other infos (useful for plotting)\n",
    "env = Monitor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:23:21.713958Z",
     "start_time": "2021-04-09T09:23:21.052616Z"
    },
    "id": "4oPTHjxyZSOL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:9.09 +/- 0.81\n"
     ]
    }
   ],
   "source": [
    "# Seed to compare to previous implementation\n",
    "env.seed(42)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5UoXTZPNdFE"
   },
   "source": [
    "## Train the agent and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:23:42.576031Z",
     "start_time": "2021-04-09T09:23:25.408346Z"
    },
    "id": "e4cfSXIB-pTF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.4     |\n",
      "|    ep_rew_mean     | 22.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 1049     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.4     |\n",
      "|    critic_loss     | 4.4      |\n",
      "|    ent_coef        | 0.0439   |\n",
      "|    ent_coef_loss   | 0.161    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19900    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 28.1        |\n",
      "|    ep_rew_mean          | 28.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 746         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010513535 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00716    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.11        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 53.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 35.8         |\n",
      "|    ep_rew_mean          | 35.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 701          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073368857 |\n",
      "|    clip_fraction        | 0.0494       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.668       |\n",
      "|    explained_variance   | 0.0673       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.7         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.014       |\n",
      "|    value_loss           | 36.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 47.6         |\n",
      "|    ep_rew_mean          | 47.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 673          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066674966 |\n",
      "|    clip_fraction        | 0.0901       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.636       |\n",
      "|    explained_variance   | 0.2          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.8         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0175      |\n",
      "|    value_loss           | 54.9         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 62.3       |\n",
      "|    ep_rew_mean          | 62.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 658        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01018395 |\n",
      "|    clip_fraction        | 0.0736     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.608     |\n",
      "|    explained_variance   | 0.34       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 21.2       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    value_loss           | 64.8       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f40f0057110>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the agent for 10000 steps\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:24:09.189433Z",
     "start_time": "2021-04-09T09:23:42.577184Z"
    },
    "id": "ygl_gVmV_QP7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:386.17 +/- 103.73\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A00W6yY3NkHG"
   },
   "source": [
    "Apparently the training went well, the mean reward increased a lot! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVm9QPNVwKXN"
   },
   "source": [
    "### Prepare video recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:24:17.837268Z",
     "start_time": "2021-04-09T09:24:17.829952Z"
    },
    "id": "MPyfQxD5z26J"
   },
   "outputs": [],
   "source": [
    "# Set up fake display; otherwise rendering will fail\n",
    "import os\n",
    "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:24:19.183229Z",
     "start_time": "2021-04-09T09:24:19.174258Z"
    },
    "id": "SLzXxO8VMD6N"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def show_videos(video_path='', prefix=''):\n",
    "  \"\"\"\n",
    "  Taken from https://github.com/eleurent/highway-env\n",
    "\n",
    "  :param video_path: (str) Path to the folder containing videos\n",
    "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
    "  \"\"\"\n",
    "  html = []\n",
    "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
    "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "      html.append('''<video alt=\"{}\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTRNUfulOGaF"
   },
   "source": [
    "We will record a video using the [VecVideoRecorder](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you can learn more about those wrappers in our Documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:24:22.831790Z",
     "start_time": "2021-04-09T09:24:22.821698Z"
    },
    "id": "Trag9dQpOIhx"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
    "  \"\"\"\n",
    "  :param env_id: (str)\n",
    "  :param model: (RL model)\n",
    "  :param video_length: (int)\n",
    "  :param prefix: (str)\n",
    "  :param video_folder: (str)\n",
    "  \"\"\"\n",
    "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
    "  # Start the video at step=0 and record 500 steps\n",
    "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
    "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
    "                              name_prefix=prefix)\n",
    "\n",
    "  obs = eval_env.reset()\n",
    "  for _ in range(video_length):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, _, _, _ = eval_env.step(action)\n",
    "\n",
    "  # Close the video recorder\n",
    "  eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOObbeu5MMlR"
   },
   "source": [
    "### Visualize trained agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:24:35.868500Z",
     "start_time": "2021-04-09T09:24:26.518387Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iATu7AiyMQW2",
    "outputId": "dcb0c463-d31c-49e0-9ea9-5cf9ae94c5d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to  /home/explore/git/guillaume/stable_baselines_3/rl-handson-rlvs21/rl-handson-rlvs21-main/videos/ppo-cartpole-step-0-to-step-500.mp4\n"
     ]
    }
   ],
   "source": [
    "record_video('CartPole-v1', model, video_length=500, prefix='ppo-cartpole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:24:40.370783Z",
     "start_time": "2021-04-09T09:24:40.360452Z"
    },
    "id": "-n4i-fW3NojZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"videos/ppo-cartpole-step-0-to-step-500.mp4\" autoplay \n",
       "                    loop controls style=\"height: 400px;\">\n",
       "                    <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAAAhtZGF0AAAA1m1vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAAAAAAAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAABidWR0YQAAAFptZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAAC1pbHN0AAAAJal0b28AAAAdZGF0YQAAAAEAAAAATGF2ZjU4LjI5LjEwMA==\" type=\"video/mp4\" />\n",
       "                </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_videos('videos', prefix='ppo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5WHto5hNaZN"
   },
   "source": [
    "### Exercise (5 minutes): Save, Load The Model and that the loading was correct\n",
    "\n",
    "Save the model and then load it.\n",
    "\n",
    "Don't forget to check that loading went well: the model must predict the same actions given the same  observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:34:50.322695Z",
     "start_time": "2021-04-09T09:34:50.301367Z"
    },
    "id": "vCcS6wJ6Nurd"
   },
   "outputs": [],
   "source": [
    "# Sample observations using the environment observation space\n",
    "observations = np.array([env.observation_space.sample() for _ in range(10)])\n",
    "# Predict actions on those observations using trained model\n",
    "\n",
    "\n",
    "action_before_saving, _ = model.predict(observations, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:34:55.275568Z",
     "start_time": "2021-04-09T09:34:55.254589Z"
    },
    "id": "BWCUZSD7NZ_Y"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:34:56.368729Z",
     "start_time": "2021-04-09T09:34:56.365527Z"
    },
    "id": "g2M1S_-V2gMi"
   },
   "outputs": [],
   "source": [
    "# Delete the model (to demonstrate loading)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:34:56.756662Z",
     "start_time": "2021-04-09T09:34:56.600899Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Ne1MlIiNpee",
    "outputId": "0350fd76-db7b-4943-c7e1-9bfa9a4c7ddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppo_cartpole.zip  sac_pendulum.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:35:03.302234Z",
     "start_time": "2021-04-09T09:35:03.271696Z"
    },
    "id": "55G-6nJLNrPk"
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = PPO.load('ppo_cartpole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:35:08.051416Z",
     "start_time": "2021-04-09T09:35:08.044769Z"
    },
    "id": "KIWII5YxNtrI"
   },
   "outputs": [],
   "source": [
    "# Predict actions on the observations with the loaded model\n",
    "action_after_loading, _ = model.predict(observations, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:35:08.477979Z",
     "start_time": "2021-04-09T09:35:08.474894Z"
    },
    "id": "m06ipphZN_0E"
   },
   "outputs": [],
   "source": [
    "# Check that the predictions are the same\n",
    "assert np.allclose(action_before_saving, action_after_loading), \"Somethng went wrong in the loading\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y8zg4V566qD"
   },
   "source": [
    "## Bonus: Train a RL Model in One Line\n",
    "\n",
    "The policy class to use will be inferred and the environment will be automatically created. This works because both are [registered](https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaOPfOrwWEP4"
   },
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fm4RncfCJFyP"
   },
   "source": [
    "# Part II: Gym Wrappers\n",
    "\n",
    "\n",
    "In this part, you will learn how to use *Gym Wrappers* which allow to do monitoring, normalization, limit the number of steps, feature augmentation, ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds4AAfmISQIA"
   },
   "source": [
    "## Anatomy of a gym wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnTS9e9hTzZZ"
   },
   "source": [
    "A gym wrapper follows the [gym](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html) interface: it has a `reset()` and `step()` method.\n",
    "\n",
    "Because a wrapper is *around* an environment, we can access it with `self.env`, this allow to easily interact with it without modifying the original env.\n",
    "There are many wrappers that have been predefined, for a complete list refer to [gym documentation](https://github.com/openai/gym/tree/master/gym/wrappers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:42:27.063520Z",
     "start_time": "2021-04-09T09:42:27.056285Z"
    },
    "id": "hYo0C0TQSL3c"
   },
   "outputs": [],
   "source": [
    "class CustomWrapper(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  :param env:  Gym environment that will be wrapped\n",
    "  \"\"\"\n",
    "  def __init__(self, env: gym.Env):\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super().__init__(env)\n",
    "  \n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment \n",
    "    \"\"\"\n",
    "    obs = self.env.reset()\n",
    "    return obs\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    :param action: ([float] or int) Action taken by the agent\n",
    "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "    \"\"\"\n",
    "    obs, reward, done, infos = self.env.step(action)\n",
    "    return obs, reward, done, infos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7RkTLshPMGq"
   },
   "source": [
    "### Exercise (7 minutes): limit the episode length\n",
    "\n",
    "In this exercise, the goal is to create a Gym wrapper that will limit the maximum number of steps per episode (timeout).\n",
    "\n",
    "\n",
    "It will also pass a `timeout` signal in the info dict to tell the agent that the termination was due to reaching the limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:48:54.788588Z",
     "start_time": "2021-04-09T09:48:54.775132Z"
    },
    "id": "4h_ypIxgPLGL"
   },
   "outputs": [],
   "source": [
    "class TimeLimitWrapper(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  Limit the maximum number of steps per episode.\n",
    "\n",
    "  :param env: Gym environment that will be wrapped\n",
    "  :param max_steps: Max number of steps per episode\n",
    "  \"\"\"\n",
    "class TimeLimitWrapper(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  Limit the maximum number of steps per episode.\n",
    "\n",
    "  :param env: Gym environment that will be wrapped\n",
    "  :param max_steps: Max number of steps per episode\n",
    "  \"\"\"\n",
    "  def __init__(self, env: gym.Env, max_steps: int = 100):\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super().__init__(env)\n",
    "    self.max_steps = max_steps\n",
    "    # YOUR CODE HERE\n",
    "    # Counter of steps per episode\n",
    "    self.counter=0\n",
    "\n",
    "    # END OF YOUR CODE\n",
    "  \n",
    "  def reset(self) -> GymObs:\n",
    "    # YOUR CODE HERE\n",
    "    # TODO: reset the counter and reset the env\n",
    "    self.counter = 0\n",
    "    self.env.reset()\n",
    "\n",
    "    # END OF YOUR CODE\n",
    "    return obs\n",
    "\n",
    "  def step(self, action: Union[int, np.ndarray]) -> Tuple[GymObs, float, bool, Dict]:\n",
    "    # YOUR CODE HERE\n",
    "    # TODO: \n",
    "    # 1. Step into the env\n",
    "    # 2. Increment the episode counter\n",
    "    # 3. Overwrite the done signal when time limit is reached \n",
    "    # (optional) 4. update the info dict (add a \"episode_timeout\" key)\n",
    "    # when the episode was stopped due to timelimit\n",
    "    obs, reward, done, infos = self.env.step(action)\n",
    "    self.counter+=1\n",
    "    if (self.counter >= self.max_steps):\n",
    "      done=True\n",
    "      infos['episode_timeout']=True\n",
    "    # END OF YOUR CODE\n",
    "    return obs, reward, done, infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FQptLSuPB3A"
   },
   "source": [
    "#### Test the wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:48:55.836546Z",
     "start_time": "2021-04-09T09:48:55.830356Z"
    },
    "id": "szZ43D5PVB07"
   },
   "outputs": [],
   "source": [
    "from gym.envs.classic_control.pendulum import PendulumEnv\n",
    "\n",
    "# Here we create the environment directly because gym.make() already wrap the environement in a TimeLimit wrapper otherwise\n",
    "env = PendulumEnv()\n",
    "# Wrap the environment\n",
    "env = TimeLimitWrapper(env, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:48:56.318876Z",
     "start_time": "2021-04-09T09:48:56.295180Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cencka9iVg9V",
    "outputId": "01ddfdb8-be4c-4df9-e052-d4a54666419e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode length: 100 steps, info dict: {'episode_timeout': True}\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "n_steps = 0\n",
    "while not done:\n",
    "  # Take random actions\n",
    "  random_action = env.action_space.sample()\n",
    "  obs, reward, done, infos = env.step(random_action)\n",
    "  n_steps += 1\n",
    "\n",
    "print(f\"Episode length: {n_steps} steps, info dict: {infos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkMYA63sV9aA"
   },
   "source": [
    "In practice, `gym` already have a wrapper for that named `TimeLimit` (`gym.wrappers.TimeLimit`) that is used by most environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yms4z_GaQ6dz"
   },
   "source": [
    "# Part III: Callbacks\n",
    "\n",
    "In this part, you will learn how to use [Callbacks](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html) which allow to do monitoring, auto saving, model manipulation, progress bars, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irHk8FXdRUnw"
   },
   "source": [
    "Please read the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html). Although Stable-Baselines3 provides you with a callback collection (e.g. for creating checkpoints or for evaluation), we are going to re-implement some so you can get a good understanding of how they work.\n",
    "\n",
    "To build a custom callback, you need to create a class that derives from `BaseCallback`. This will give you access to events (`_on_training_start`, `_on_step()`) and useful variables (like `self.model` for the RL model).\n",
    "\n",
    "`_on_step` returns a boolean value for whether or not the training should continue.\n",
    "\n",
    "Thanks to the access to the models variables, in particular `self.model`, we are able to even change the parameters of the model without halting the training, or changing the model's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:51:57.357620Z",
     "start_time": "2021-04-09T09:51:57.350150Z"
    },
    "id": "uE30k2i7kohh"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjRvJ8zBftL3"
   },
   "outputs": [],
   "source": [
    "class CustomCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseRLModel\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = None  # type: Dict[str, Any]\n",
    "        # self.globals = None  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger = None  # type: logger.Logger\n",
    "        # # Sometimes, for event callback, it is useful\n",
    "        # # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqpPtxaCfynB"
   },
   "source": [
    "Here we have a simple callback that can only be called twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:52:23.479041Z",
     "start_time": "2021-04-09T09:52:23.470305Z"
    },
    "id": "7ILY0AkFfzPJ"
   },
   "outputs": [],
   "source": [
    "class SimpleCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    a simple callback that can only be called twice\n",
    "\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super(SimpleCallback, self).__init__(verbose)\n",
    "        self._called = False\n",
    "    \n",
    "    def _on_step(self):\n",
    "      \n",
    "      if not self._called:\n",
    "        print(\"callback - first call\")\n",
    "        self._called = True\n",
    "        return True # returns True, training continues.\n",
    "\n",
    "      print(\"callback - second call\")\n",
    "      return False # returns False, training stops.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T09:52:24.203757Z",
     "start_time": "2021-04-09T09:52:24.157610Z"
    },
    "id": "5gTXaNLARUnw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Creating environment from the given name 'Pendulum-v0'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "callback - first call\n",
      "callback - second call\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x7f4089e5b790>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SAC('MlpPolicy', 'Pendulum-v0', verbose=1)\n",
    "model.learn(8000, callback=SimpleCallback())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adsKMvDkRUn0"
   },
   "source": [
    "## Exercise (8 minutes): Checkpoint Callback\n",
    "\n",
    "In RL, it is quite useful to save checkpoints during training, as we can end up with burn-in of a bad policy. It also useful if you want to see the progression over time.\n",
    "\n",
    "This is a typical use case for callback, as they can call the save function of the model, and observe the training over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDI3lKTiiKP9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T10:00:17.392097Z",
     "start_time": "2021-04-09T10:00:17.381668Z"
    },
    "id": "nzMHj7r3h78m"
   },
   "outputs": [],
   "source": [
    "class CheckpointCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model every ``save_freq`` steps\n",
    "\n",
    "    :param save_freq:\n",
    "    :param save_path: Path to the folder where the model will be saved.\n",
    "    :param name_prefix: Common prefix to the saved models\n",
    "    :param verbose: Whether to print additional infos or not\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, save_freq: int, save_path: str, name_prefix: str = \"rl_model\", verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.save_freq = save_freq\n",
    "        self.save_path = save_path\n",
    "        self.name_prefix = name_prefix\n",
    "        # NOTE: because it derives from `BaseCallback`\n",
    "        # this checkpoint callback has already access to many variables\n",
    "        # like `self.model` (cf ``CustomCallback above for a complete list)\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        ## YOUR CODE HERE\n",
    "        # Create folder if needed\n",
    "        # (you may use `os.makedirs()`)\n",
    "        os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "        ## END OF YOUR CODE\n",
    "\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        ## YOUR CODE HERE\n",
    "        # Save the checkpoint if needed\n",
    "        if (self.num_timesteps % self.save_freq ==0):\n",
    "          print('save model')\n",
    "          self.model.save(self.save_path+self.name_prefix+'_'+str(self.num_timesteps))\n",
    "\n",
    "        ## END OF YOUR CODE\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3krf5FCaLYn"
   },
   "source": [
    "Test your callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T10:00:29.029488Z",
     "start_time": "2021-04-09T10:00:18.940169Z"
    },
    "id": "1TuYLBEaRUn0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 24.6        |\n",
      "|    ep_rew_mean          | 24.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 503         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 500         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005439931 |\n",
      "|    clip_fraction        | 0.0454      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.69       |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.0007      |\n",
      "|    loss                 | 21.4        |\n",
      "|    n_updates            | 99          |\n",
      "|    policy_gradient_loss | -0.00985    |\n",
      "|    policy_loss          | -2.03       |\n",
      "|    value_loss           | 24.7        |\n",
      "-----------------------------------------\n",
      "save model\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 27       |\n",
      "|    ep_rew_mean        | 27       |\n",
      "| time/                 |          |\n",
      "|    fps                | 495      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.664   |\n",
      "|    explained_variance | 0.034    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.25     |\n",
      "|    value_loss         | 7.02     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 28.7     |\n",
      "|    ep_rew_mean        | 28.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 499      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.659   |\n",
      "|    explained_variance | 0.0219   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 1.51     |\n",
      "|    value_loss         | 6.25     |\n",
      "------------------------------------\n",
      "save model\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 33.8      |\n",
      "|    ep_rew_mean        | 33.8      |\n",
      "| time/                 |           |\n",
      "|    fps                | 486       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.668    |\n",
      "|    explained_variance | -0.000447 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 1.11      |\n",
      "|    value_loss         | 5.84      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 37       |\n",
      "|    ep_rew_mean        | 37       |\n",
      "| time/                 |          |\n",
      "|    fps                | 486      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.687   |\n",
      "|    explained_variance | -0.00363 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 1.24     |\n",
      "|    value_loss         | 5.28     |\n",
      "------------------------------------\n",
      "save model\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 37.9     |\n",
      "|    ep_rew_mean        | 37.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 490      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.592   |\n",
      "|    explained_variance | -0.0199  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.714    |\n",
      "|    value_loss         | 4.72     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 40.8      |\n",
      "|    ep_rew_mean        | 40.8      |\n",
      "| time/                 |           |\n",
      "|    fps                | 494       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.633    |\n",
      "|    explained_variance | -0.000176 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 0.822     |\n",
      "|    value_loss         | 4.16      |\n",
      "-------------------------------------\n",
      "save model\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 43.6     |\n",
      "|    ep_rew_mean        | 43.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 495      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.61    |\n",
      "|    explained_variance | 0.00184  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 1.01     |\n",
      "|    value_loss         | 3.64     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 45.4     |\n",
      "|    ep_rew_mean        | 45.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 497      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.579   |\n",
      "|    explained_variance | 0.00116  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.685    |\n",
      "|    value_loss         | 3.16     |\n",
      "------------------------------------\n",
      "save model\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 48.6     |\n",
      "|    ep_rew_mean        | 48.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 496      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.541   |\n",
      "|    explained_variance | 0.000121 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.652    |\n",
      "|    value_loss         | 2.7      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7f40f00e1c50>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = \"./tmp/gym/\"\n",
    "# Create Callback\n",
    "callback = CheckpointCallback(save_freq=1000, save_path=\"./tmp/gym/\", verbose=1)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", \"CartPole-v1\", verbose=1)\n",
    "model.learn(total_timesteps=5000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T10:00:29.170610Z",
     "start_time": "2021-04-09T10:00:29.030515Z"
    },
    "id": "Yc_z8jF-TFQP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rl_model_1000.zip  rl_model_3000.zip  rl_model_5000.zip\r\n",
      "rl_model_2000.zip  rl_model_4000.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"./tmp/gym/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0_iiocKUTW0"
   },
   "source": [
    "Note: The `CheckpointCallback` as well as other [common callbacks](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html), like the `EvalCallback` are already included in Stable-Baselines3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4TavoikVlCt"
   },
   "source": [
    "## Multiprocessing Demo\n",
    "\n",
    "\n",
    "[Vectorized Environments](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html) are a method for stacking multiple independent environments into a single environment. Instead of training an RL agent on 1 environment per step, it allows us to train it on n environments per step. This provides two benefits:\n",
    "* Agent experience can be collected more quickly\n",
    "* The experience will contain a more diverse range of states, it usually improves exploration\n",
    "\n",
    "Stable-Baselines provides two types of Vectorized Environment:\n",
    "- SubprocVecEnv which run each environment in a separate process\n",
    "- DummyVecEnv which run all environment on the same process\n",
    "\n",
    "In practice, DummyVecEnv is usually faster than SubprocVecEnv because of communication delays that subprocesses have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T10:13:19.376381Z",
     "start_time": "2021-04-09T10:13:19.372121Z"
    },
    "id": "7obQMBgXV8_z"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T10:13:19.843997Z",
     "start_time": "2021-04-09T10:13:19.839436Z"
    },
    "id": "S1K_YuEfVnT6"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "n_steps = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T10:13:44.314367Z",
     "start_time": "2021-04-09T10:13:21.825308Z"
    },
    "id": "fBa5rJfiWFX3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.28e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 984       |\n",
      "|    iterations         | 1         |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1024      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.598    |\n",
      "|    explained_variance | 0.000259  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1000      |\n",
      "|    policy_loss        | 0.633     |\n",
      "|    value_loss         | 2.7       |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.3e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 909          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | -9.62097e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.00218      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.16e+03     |\n",
      "|    n_updates            | 1            |\n",
      "|    policy_gradient_loss | -0.000103    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.1e+04      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.38e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 908           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 3072          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013049104 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.00738       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.81e+03      |\n",
      "|    n_updates            | 2             |\n",
      "|    policy_gradient_loss | 0.000201      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.1e+04       |\n",
      "-------------------------------------------\n",
      "---------------------------------------------\n",
      "| rollout/                |                 |\n",
      "|    ep_len_mean          | 200             |\n",
      "|    ep_rew_mean          | -1.32e+03       |\n",
      "| time/                   |                 |\n",
      "|    fps                  | 914             |\n",
      "|    iterations           | 4               |\n",
      "|    time_elapsed         | 4               |\n",
      "|    total_timesteps      | 4096            |\n",
      "| train/                  |                 |\n",
      "|    approx_kl            | -0.000114500814 |\n",
      "|    clip_fraction        | 0               |\n",
      "|    clip_range           | 0.2             |\n",
      "|    entropy_loss         | -1.42           |\n",
      "|    explained_variance   | 0.00894         |\n",
      "|    learning_rate        | 0.0003          |\n",
      "|    loss                 | 7.2e+03         |\n",
      "|    n_updates            | 3               |\n",
      "|    policy_gradient_loss | -6.11e-05       |\n",
      "|    std                  | 1               |\n",
      "|    value_loss           | 1.46e+04        |\n",
      "---------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.29e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 919           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 5120          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.1809882e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0263        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.63e+03      |\n",
      "|    n_updates            | 4             |\n",
      "|    policy_gradient_loss | -0.000374     |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 8.87e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.3e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 919           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00061551447 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0244        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.5e+03       |\n",
      "|    n_updates            | 5             |\n",
      "|    policy_gradient_loss | 0.000477      |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 9e+03         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.31e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 919           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 7             |\n",
      "|    total_timesteps      | 7168          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2346405e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0211        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.88e+03      |\n",
      "|    n_updates            | 6             |\n",
      "|    policy_gradient_loss | 3.94e-05      |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 1.14e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.27e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 918           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 8             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2204051e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0412        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.68e+03      |\n",
      "|    n_updates            | 7             |\n",
      "|    policy_gradient_loss | 0.000131      |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 1.18e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.25e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 919           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 9216          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -8.522981e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0363        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.11e+03      |\n",
      "|    n_updates            | 8             |\n",
      "|    policy_gradient_loss | -7.7e-05      |\n",
      "|    std                  | 0.997         |\n",
      "|    value_loss           | 5.75e+03      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.26e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 919           |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 11            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016592455 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0703        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.62e+03      |\n",
      "|    n_updates            | 9             |\n",
      "|    policy_gradient_loss | 7.71e-05      |\n",
      "|    std                  | 0.997         |\n",
      "|    value_loss           | 8.99e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.25e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 918           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 12            |\n",
      "|    total_timesteps      | 11264         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5959394e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.069         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.86e+03      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -3.49e-05     |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 1.12e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.24e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 915           |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 13            |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031077105 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0747        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.15e+03      |\n",
      "|    n_updates            | 11            |\n",
      "|    policy_gradient_loss | 0.000117      |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 9.53e+03      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.25e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 914         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000166893 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.072       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.84e+03    |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | 6.67e-05    |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 7.8e+03     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.24e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 911           |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 15            |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -1.930748e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0885        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.67e+03      |\n",
      "|    n_updates            | 13            |\n",
      "|    policy_gradient_loss | 1.33e-05      |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 1.06e+04      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 200            |\n",
      "|    ep_rew_mean          | -1.24e+03      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 910            |\n",
      "|    iterations           | 15             |\n",
      "|    time_elapsed         | 16             |\n",
      "|    total_timesteps      | 15360          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -6.1902625e-05 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | 0.1            |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 4.08e+03       |\n",
      "|    n_updates            | 14             |\n",
      "|    policy_gradient_loss | -0.000178      |\n",
      "|    std                  | 0.998          |\n",
      "|    value_loss           | 8.88e+03       |\n",
      "--------------------------------------------\n",
      "---------------------------------------------\n",
      "| rollout/                |                 |\n",
      "|    ep_len_mean          | 200             |\n",
      "|    ep_rew_mean          | -1.22e+03       |\n",
      "| time/                   |                 |\n",
      "|    fps                  | 914             |\n",
      "|    iterations           | 16              |\n",
      "|    time_elapsed         | 17              |\n",
      "|    total_timesteps      | 16384           |\n",
      "| train/                  |                 |\n",
      "|    approx_kl            | -0.000121842604 |\n",
      "|    clip_fraction        | 0               |\n",
      "|    clip_range           | 0.2             |\n",
      "|    entropy_loss         | -1.42           |\n",
      "|    explained_variance   | 0.105           |\n",
      "|    learning_rate        | 0.0003          |\n",
      "|    loss                 | 4.54e+03        |\n",
      "|    n_updates            | 15              |\n",
      "|    policy_gradient_loss | 0.00013         |\n",
      "|    std                  | 1               |\n",
      "|    value_loss           | 8.4e+03         |\n",
      "---------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 200            |\n",
      "|    ep_rew_mean          | -1.23e+03      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 914            |\n",
      "|    iterations           | 17             |\n",
      "|    time_elapsed         | 19             |\n",
      "|    total_timesteps      | 17408          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -5.2524847e-05 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | 0.0701         |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 3.51e+03       |\n",
      "|    n_updates            | 16             |\n",
      "|    policy_gradient_loss | 9.6e-05        |\n",
      "|    std                  | 1              |\n",
      "|    value_loss           | 6.2e+03        |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.23e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 914           |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 20            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -6.601331e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0901        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.13e+03      |\n",
      "|    n_updates            | 17            |\n",
      "|    policy_gradient_loss | -7.92e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.15e+04      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.22e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 915           |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 21            |\n",
      "|    total_timesteps      | 19456         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022759824 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.121         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.37e+03      |\n",
      "|    n_updates            | 18            |\n",
      "|    policy_gradient_loss | -0.000376     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.47e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.21e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 915           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 22            |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011646398 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.135         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.78e+03      |\n",
      "|    n_updates            | 19            |\n",
      "|    policy_gradient_loss | 9.63e-05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.47e+03      |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start_time_one_env = time.time()\n",
    "model = PPO(\"MlpPolicy\", env, n_epochs=1, n_steps=n_steps, verbose=1).learn(int(2e4))\n",
    "time_one_env = time.time() - start_time_one_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T10:13:44.317710Z",
     "start_time": "2021-04-09T10:13:44.315415Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ylbC5slbWzKo",
    "outputId": "2f7cc82e-224d-4d5a-8803-a31a9013cf71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 22.48s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Took {time_one_env:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T10:13:51.102887Z",
     "start_time": "2021-04-09T10:13:46.668785Z"
    },
    "id": "n0UgmDOHWBWU",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 6787          |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 0             |\n",
      "|    total_timesteps      | 1024          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026454462 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.129         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.65e+03      |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | 0.000192      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.39e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 5530         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008703073 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.000881     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.05e+03     |\n",
      "|    n_updates            | 1            |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.73e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 5197          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 0             |\n",
      "|    total_timesteps      | 3072          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -7.897202e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.00553       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.62e+03      |\n",
      "|    n_updates            | 2             |\n",
      "|    policy_gradient_loss | 0.00026       |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 9.06e+03      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 200            |\n",
      "|    ep_rew_mean          | -1.34e+03      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 5039           |\n",
      "|    iterations           | 4              |\n",
      "|    time_elapsed         | 0              |\n",
      "|    total_timesteps      | 4096           |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00024498178 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | 0.0117         |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 4.6e+03        |\n",
      "|    n_updates            | 3              |\n",
      "|    policy_gradient_loss | 5.99e-05       |\n",
      "|    std                  | 0.999          |\n",
      "|    value_loss           | 8.9e+03        |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.34e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 4993          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 1             |\n",
      "|    total_timesteps      | 5120          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029603235 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0118        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.09e+03      |\n",
      "|    n_updates            | 4             |\n",
      "|    policy_gradient_loss | 0.000411      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.79e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.34e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 4956          |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 1             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012995955 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0201        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.34e+03      |\n",
      "|    n_updates            | 5             |\n",
      "|    policy_gradient_loss | 8.98e-05      |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 6.87e+03      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 200            |\n",
      "|    ep_rew_mean          | -1.27e+03      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 4937           |\n",
      "|    iterations           | 7              |\n",
      "|    time_elapsed         | 1              |\n",
      "|    total_timesteps      | 7168           |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -9.9351164e-05 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | 0.0223         |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 3.32e+03       |\n",
      "|    n_updates            | 6              |\n",
      "|    policy_gradient_loss | -2.6e-05       |\n",
      "|    std                  | 0.998          |\n",
      "|    value_loss           | 6.91e+03       |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.27e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 4902          |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 1             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.4361684e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0247        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.92e+03      |\n",
      "|    n_updates            | 7             |\n",
      "|    policy_gradient_loss | -9.98e-05     |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 5.45e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.27e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4886         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 9216         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003103289 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0388       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.91e+03     |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | 0.000337     |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 7.42e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.26e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 4867          |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011599128 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.032         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.63e+03      |\n",
      "|    n_updates            | 9             |\n",
      "|    policy_gradient_loss | 0.00017       |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 7.88e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.26e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4852         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 11264        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002939421 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0447       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.29e+03     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 0.000139     |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 4.51e+03     |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 200            |\n",
      "|    ep_rew_mean          | -1.26e+03      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 4832           |\n",
      "|    iterations           | 12             |\n",
      "|    time_elapsed         | 2              |\n",
      "|    total_timesteps      | 12288          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00013581937 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | 0.065          |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 3.36e+03       |\n",
      "|    n_updates            | 11             |\n",
      "|    policy_gradient_loss | 3.07e-05       |\n",
      "|    std                  | 0.998          |\n",
      "|    value_loss           | 6.26e+03       |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.24e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4801         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 13312        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.755816e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0579       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.24e+03     |\n",
      "|    n_updates            | 12           |\n",
      "|    policy_gradient_loss | -5.43e-05    |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 7.08e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.24e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 4711          |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -4.908303e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0653        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.1e+03       |\n",
      "|    n_updates            | 13            |\n",
      "|    policy_gradient_loss | -1.09e-05     |\n",
      "|    std                  | 0.997         |\n",
      "|    value_loss           | 4.13e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.24e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4708         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 15360        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003736497 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0757       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.02e+03     |\n",
      "|    n_updates            | 14           |\n",
      "|    policy_gradient_loss | 6.89e-05     |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 6.08e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.22e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 4714          |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.5290468e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0649        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.03e+03      |\n",
      "|    n_updates            | 15            |\n",
      "|    policy_gradient_loss | 7.89e-05      |\n",
      "|    std                  | 0.997         |\n",
      "|    value_loss           | 6.23e+03      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.22e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4723        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.30205e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.0576      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.24e+03    |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | 3.31e-05    |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 4.45e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.22e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 4732          |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -6.847491e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0659        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.27e+03      |\n",
      "|    n_updates            | 17            |\n",
      "|    policy_gradient_loss | 1.42e-05      |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 8.81e+03      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 200            |\n",
      "|    ep_rew_mean          | -1.25e+03      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 4723           |\n",
      "|    iterations           | 19             |\n",
      "|    time_elapsed         | 4              |\n",
      "|    total_timesteps      | 19456          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -2.4154782e-05 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | 0.0492         |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 5.21e+03       |\n",
      "|    n_updates            | 18             |\n",
      "|    policy_gradient_loss | 5.64e-05       |\n",
      "|    std                  | 0.998          |\n",
      "|    value_loss           | 9.14e+03       |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.25e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 4737          |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2651162e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0561        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.05e+03      |\n",
      "|    n_updates            | 19            |\n",
      "|    policy_gradient_loss | 6.7e-06       |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 6.22e+03      |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start_time_vec_env = time.time()\n",
    "# Create 16 environments\n",
    "vec_env = make_vec_env(\"Pendulum-v0\", n_envs=16)\n",
    "# At each call to `env.step()`, 16 transitions will be collected, so we account for that for fair comparison\n",
    "model = PPO(\"MlpPolicy\", vec_env, n_epochs=1, n_steps=n_steps // 16, verbose=1).learn(int(2e4))\n",
    "\n",
    "time_vec_env = time.time() - start_time_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T10:13:52.646029Z",
     "start_time": "2021-04-09T10:13:52.640371Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lN3o5TJ3WxOy",
    "outputId": "0018d2aa-a8b0-474a-a23f-b4151debba42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 4.43s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Took {time_vec_env:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUrDxSkJT_gn"
   },
   "source": [
    "Note: the speedup is not linear but it is already significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwFOp0j-ga-_"
   },
   "source": [
    "# Part IV: The importance of hyperparameter tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PytOtL9GdmrE"
   },
   "source": [
    "When compared with Supervised Learning, Deep Reinforcement Learning is far more sensitive to the choice of hyper-parameters such as learning rate, number of neurons, number of layers, optimizer ... etc. \n",
    "\n",
    "Poor choice of hyper-parameters can lead to poor/unstable convergence. This challenge is compounded by the variability in performance across random seeds (used to initialize the network weights and the environment).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8PNN9kcgolk"
   },
   "source": [
    "### Challenge (15 minutes): \"Grad Student Descent\" - Can you beat automatic hyperparameter tuning?\n",
    "\n",
    "The challenge is to find the best hyperparameters (max performance) for A2C on `CartPole-v1` with a limited budget of 20 000 training steps.\n",
    "\n",
    "You will compete against automatic hyperparameter tuning, good luck ;)\n",
    "\n",
    "\n",
    "Maximum reward: 500 on `CartPole-v1`\n",
    "\n",
    "The hyperparameters should work for different random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T10:18:49.189195Z",
     "start_time": "2021-04-09T10:18:49.187310Z"
    },
    "id": "s6aqxsini7H3"
   },
   "outputs": [],
   "source": [
    "budget = int(2e4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDQ805DBi3KM"
   },
   "source": [
    "#### The baseline: default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T10:19:29.817188Z",
     "start_time": "2021-04-09T10:18:50.022367Z"
    },
    "id": "D1PSNGcsi2dP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 22.9         |\n",
      "|    ep_rew_mean          | 22.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 487          |\n",
      "|    iterations           | 100          |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 500          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.901735e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.692       |\n",
      "|    explained_variance   | -0.393       |\n",
      "|    learning_rate        | 0.0007       |\n",
      "|    loss                 | 4.38e+03     |\n",
      "|    n_updates            | 99           |\n",
      "|    policy_gradient_loss | 9.49e-05     |\n",
      "|    policy_loss          | 1.85         |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 10.4         |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 26.1     |\n",
      "|    ep_rew_mean        | 26.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 495      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.682   |\n",
      "|    explained_variance | 0.0378   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.61     |\n",
      "|    value_loss         | 7.07     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 31       |\n",
      "|    ep_rew_mean        | 31       |\n",
      "| time/                 |          |\n",
      "|    fps                | 500      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.622   |\n",
      "|    explained_variance | -0.186   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 1.18     |\n",
      "|    value_loss         | 7        |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 33.5     |\n",
      "|    ep_rew_mean        | 33.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 489      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.611   |\n",
      "|    explained_variance | 0.0337   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.774    |\n",
      "|    value_loss         | 5.78     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 36.7     |\n",
      "|    ep_rew_mean        | 36.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 492      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.572   |\n",
      "|    explained_variance | 0.0122   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -7.47    |\n",
      "|    value_loss         | 565      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 37.9     |\n",
      "|    ep_rew_mean        | 37.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 488      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.509   |\n",
      "|    explained_variance | -0.017   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 1.58     |\n",
      "|    value_loss         | 4.71     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 41.1     |\n",
      "|    ep_rew_mean        | 41.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 493      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.516   |\n",
      "|    explained_variance | 0.00175  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 1.13     |\n",
      "|    value_loss         | 4.25     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 44.3     |\n",
      "|    ep_rew_mean        | 44.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 497      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.588   |\n",
      "|    explained_variance | 0.00309  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 1.01     |\n",
      "|    value_loss         | 3.69     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 48.8     |\n",
      "|    ep_rew_mean        | 48.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 496      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.521   |\n",
      "|    explained_variance | 0.00023  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.427    |\n",
      "|    value_loss         | 3.19     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 51.4      |\n",
      "|    ep_rew_mean        | 51.4      |\n",
      "| time/                 |           |\n",
      "|    fps                | 494       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.529    |\n",
      "|    explained_variance | -8.42e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 0.567     |\n",
      "|    value_loss         | 2.72      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 54        |\n",
      "|    ep_rew_mean        | 54        |\n",
      "| time/                 |           |\n",
      "|    fps                | 496       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.414    |\n",
      "|    explained_variance | -0.000824 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 0.996     |\n",
      "|    value_loss         | 2.3       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 57.8     |\n",
      "|    ep_rew_mean        | 57.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 497      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.295   |\n",
      "|    explained_variance | 0.001    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.956    |\n",
      "|    value_loss         | 1.9      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 61.8      |\n",
      "|    ep_rew_mean        | 61.8      |\n",
      "| time/                 |           |\n",
      "|    fps                | 499       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.469    |\n",
      "|    explained_variance | -0.000109 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 0.333     |\n",
      "|    value_loss         | 1.54      |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 66.1      |\n",
      "|    ep_rew_mean        | 66.1      |\n",
      "| time/                 |           |\n",
      "|    fps                | 500       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.425    |\n",
      "|    explained_variance | -9.72e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 0.279     |\n",
      "|    value_loss         | 1.21      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 70.8     |\n",
      "|    ep_rew_mean        | 70.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 501      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.485   |\n",
      "|    explained_variance | 0.000126 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.319    |\n",
      "|    value_loss         | 0.932    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 75.2      |\n",
      "|    ep_rew_mean        | 75.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 501       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.399    |\n",
      "|    explained_variance | -0.000137 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 0.294     |\n",
      "|    value_loss         | 0.679     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 77.9     |\n",
      "|    ep_rew_mean        | 77.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 499      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.388   |\n",
      "|    explained_variance | 0.000154 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.246    |\n",
      "|    value_loss         | 0.483    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 81.2     |\n",
      "|    ep_rew_mean        | 81.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 498      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.455   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.134    |\n",
      "|    value_loss         | 0.31     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 85.6      |\n",
      "|    ep_rew_mean        | 85.6      |\n",
      "| time/                 |           |\n",
      "|    fps                | 497       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.325    |\n",
      "|    explained_variance | -0.000134 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 0.0903    |\n",
      "|    value_loss         | 0.176     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 89.8      |\n",
      "|    ep_rew_mean        | 89.8      |\n",
      "| time/                 |           |\n",
      "|    fps                | 496       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.476    |\n",
      "|    explained_variance | -3.93e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -7.54     |\n",
      "|    value_loss         | 1.64e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 92.1     |\n",
      "|    ep_rew_mean        | 92.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 497      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.451   |\n",
      "|    explained_variance | 0.000173 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 0.0668   |\n",
      "|    value_loss         | 0.0217   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 95       |\n",
      "|    ep_rew_mean        | 95       |\n",
      "| time/                 |          |\n",
      "|    fps                | 498      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.388   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | 0.0137   |\n",
      "|    value_loss         | 0.000776 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 99.4     |\n",
      "|    ep_rew_mean        | 99.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 500      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.334   |\n",
      "|    explained_variance | 1.38e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 0.0015   |\n",
      "|    value_loss         | 8.5e-06  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 102      |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 501      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.422   |\n",
      "|    explained_variance | -0.00518 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 0.000694 |\n",
      "|    value_loss         | 3.87e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 105      |\n",
      "|    ep_rew_mean        | 105      |\n",
      "| time/                 |          |\n",
      "|    fps                | 500      |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.398   |\n",
      "|    explained_variance | 2.52e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 0.00021  |\n",
      "|    value_loss         | 5.41e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 108      |\n",
      "|    ep_rew_mean        | 108      |\n",
      "| time/                 |          |\n",
      "|    fps                | 501      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.45    |\n",
      "|    explained_variance | -0.0075  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 2.99e-05 |\n",
      "|    value_loss         | 1.1e-08  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 111      |\n",
      "|    ep_rew_mean        | 111      |\n",
      "| time/                 |          |\n",
      "|    fps                | 503      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.384   |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 1.16e-11 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 115      |\n",
      "|    ep_rew_mean        | 115      |\n",
      "| time/                 |          |\n",
      "|    fps                | 503      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.416   |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 8.49e-07 |\n",
      "|    value_loss         | 1.98e-10 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 122      |\n",
      "|    ep_rew_mean        | 122      |\n",
      "| time/                 |          |\n",
      "|    fps                | 501      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.351   |\n",
      "|    explained_variance | 0.0098   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 0.00204  |\n",
      "|    value_loss         | 1.31e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 125      |\n",
      "|    ep_rew_mean        | 125      |\n",
      "| time/                 |          |\n",
      "|    fps                | 502      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.172   |\n",
      "|    explained_variance | -0.0138  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 0.000535 |\n",
      "|    value_loss         | 1.12e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 128      |\n",
      "|    ep_rew_mean        | 128      |\n",
      "| time/                 |          |\n",
      "|    fps                | 501      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.434   |\n",
      "|    explained_variance | -0.00549 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | 0.000923 |\n",
      "|    value_loss         | 3.51e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 131      |\n",
      "|    ep_rew_mean        | 131      |\n",
      "| time/                 |          |\n",
      "|    fps                | 503      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.446   |\n",
      "|    explained_variance | -0.0093  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | 0.000337 |\n",
      "|    value_loss         | 1.2e-06  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 134      |\n",
      "|    ep_rew_mean        | 134      |\n",
      "| time/                 |          |\n",
      "|    fps                | 503      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.367   |\n",
      "|    explained_variance | 4.05e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 0.00219  |\n",
      "|    value_loss         | 1.29e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 137       |\n",
      "|    ep_rew_mean        | 137       |\n",
      "| time/                 |           |\n",
      "|    fps                | 503       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.489    |\n",
      "|    explained_variance | -3.91e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 0.00121   |\n",
      "|    value_loss         | 1.84e-05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 137      |\n",
      "|    ep_rew_mean        | 137      |\n",
      "| time/                 |          |\n",
      "|    fps                | 503      |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 17500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.424   |\n",
      "|    explained_variance | -0.007   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | 0.000596 |\n",
      "|    value_loss         | 8.54e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 138      |\n",
      "|    ep_rew_mean        | 138      |\n",
      "| time/                 |          |\n",
      "|    fps                | 502      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.472   |\n",
      "|    explained_variance | -0.00956 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 0.00171  |\n",
      "|    value_loss         | 1.41e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 138      |\n",
      "|    ep_rew_mean        | 138      |\n",
      "| time/                 |          |\n",
      "|    fps                | 501      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.504   |\n",
      "|    explained_variance | 3.34e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | 0.000238 |\n",
      "|    value_loss         | 3.82e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 140      |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 502      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.485   |\n",
      "|    explained_variance | -0.00155 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 0.00179  |\n",
      "|    value_loss         | 1.04e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 142      |\n",
      "|    ep_rew_mean        | 142      |\n",
      "| time/                 |          |\n",
      "|    fps                | 502      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 38       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.43    |\n",
      "|    explained_variance | 4.91e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.000321 |\n",
      "|    value_loss         | 2.38e-06 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 144      |\n",
      "|    ep_rew_mean        | 144      |\n",
      "| time/                 |          |\n",
      "|    fps                | 502      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.453   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 1.28e-05 |\n",
      "|    value_loss         | 2.31e-09 |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = A2C(\"MlpPolicy\", \"CartPole-v1\", seed=8, verbose=1).learn(budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T10:19:36.288323Z",
     "start_time": "2021-04-09T10:19:29.818201Z"
    },
    "id": "2d3X0G0ng2OE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:181.50 +/- 19.02\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=50, deterministic=True)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-fi1-oKnUI2"
   },
   "source": [
    "**Your goal is to beat that baseline and get closer to the optimal score of 500**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvq8zizok1X_"
   },
   "source": [
    "Time to tune!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T10:21:14.566907Z",
     "start_time": "2021-04-09T10:21:14.563402Z"
    },
    "id": "UaqCCH4gkRH_"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T10:21:53.541857Z",
     "start_time": "2021-04-09T10:21:15.057253Z"
    },
    "id": "uDUfeZcyjPKS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 22.9     |\n",
      "|    ep_rew_mean        | 22.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 515      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.692   |\n",
      "|    explained_variance | -0.356   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 2.04     |\n",
      "|    value_loss         | 12.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 28.5     |\n",
      "|    ep_rew_mean        | 28.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 523      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.681   |\n",
      "|    explained_variance | 0.0419   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.89     |\n",
      "|    value_loss         | 10.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 29.7     |\n",
      "|    ep_rew_mean        | 29.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 522      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.611   |\n",
      "|    explained_variance | -0.253   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 1.48     |\n",
      "|    value_loss         | 11.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 29.2     |\n",
      "|    ep_rew_mean        | 29.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 508      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.645   |\n",
      "|    explained_variance | 0.0391   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 1.27     |\n",
      "|    value_loss         | 10.5     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 31.1     |\n",
      "|    ep_rew_mean        | 31.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 505      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.62    |\n",
      "|    explained_variance | 0.00146  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 1.86     |\n",
      "|    value_loss         | 10.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 34.5     |\n",
      "|    ep_rew_mean        | 34.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 508      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.481   |\n",
      "|    explained_variance | -0.0117  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 2.59     |\n",
      "|    value_loss         | 10.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 37.9     |\n",
      "|    ep_rew_mean        | 37.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 507      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.6     |\n",
      "|    explained_variance | 0.00197  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 1.89     |\n",
      "|    value_loss         | 10.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 40.2     |\n",
      "|    ep_rew_mean        | 40.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 505      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.599   |\n",
      "|    explained_variance | 0.0031   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 1.94     |\n",
      "|    value_loss         | 10.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 43.1      |\n",
      "|    ep_rew_mean        | 43.1      |\n",
      "| time/                 |           |\n",
      "|    fps                | 508       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.611    |\n",
      "|    explained_variance | -0.000619 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 1.76      |\n",
      "|    value_loss         | 10.9      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 45.7     |\n",
      "|    ep_rew_mean        | 45.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 511      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.54    |\n",
      "|    explained_variance | 0.000272 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 1.33     |\n",
      "|    value_loss         | 10.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 48.8     |\n",
      "|    ep_rew_mean        | 48.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 512      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.627   |\n",
      "|    explained_variance | 0.000215 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 1.28     |\n",
      "|    value_loss         | 10.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 52.3     |\n",
      "|    ep_rew_mean        | 52.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 513      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.6     |\n",
      "|    explained_variance | 0.000327 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 1.85     |\n",
      "|    value_loss         | 10.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 55.8     |\n",
      "|    ep_rew_mean        | 55.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 514      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.564   |\n",
      "|    explained_variance | 8.3e-05  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 1.18     |\n",
      "|    value_loss         | 10.9     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 59.1      |\n",
      "|    ep_rew_mean        | 59.1      |\n",
      "| time/                 |           |\n",
      "|    fps                | 512       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.563    |\n",
      "|    explained_variance | -5.23e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 1.59      |\n",
      "|    value_loss         | 10.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 62.8      |\n",
      "|    ep_rew_mean        | 62.8      |\n",
      "| time/                 |           |\n",
      "|    fps                | 514       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.629    |\n",
      "|    explained_variance | -2.46e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 1.48      |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 67.7      |\n",
      "|    ep_rew_mean        | 67.7      |\n",
      "| time/                 |           |\n",
      "|    fps                | 516       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.328    |\n",
      "|    explained_variance | -5.69e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 2.73      |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 70        |\n",
      "|    ep_rew_mean        | 70        |\n",
      "| time/                 |           |\n",
      "|    fps                | 516       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.525    |\n",
      "|    explained_variance | -3.23e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 0.884     |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 72.5      |\n",
      "|    ep_rew_mean        | 72.5      |\n",
      "| time/                 |           |\n",
      "|    fps                | 514       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.581    |\n",
      "|    explained_variance | -7.75e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 1.47      |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 77.4     |\n",
      "|    ep_rew_mean        | 77.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 515      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 2.04     |\n",
      "|    value_loss         | 10.8     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 81.9      |\n",
      "|    ep_rew_mean        | 81.9      |\n",
      "| time/                 |           |\n",
      "|    fps                | 513       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.598    |\n",
      "|    explained_variance | -2.98e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 2.11      |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 86.6     |\n",
      "|    ep_rew_mean        | 86.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 514      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.618   |\n",
      "|    explained_variance | 7.69e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 1.5      |\n",
      "|    value_loss         | 10.8     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 90        |\n",
      "|    ep_rew_mean        | 90        |\n",
      "| time/                 |           |\n",
      "|    fps                | 514       |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.507    |\n",
      "|    explained_variance | -3.22e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | 0.941     |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 93.1     |\n",
      "|    ep_rew_mean        | 93.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 514      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.633   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 1.29     |\n",
      "|    value_loss         | 10.8     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 97.6      |\n",
      "|    ep_rew_mean        | 97.6      |\n",
      "| time/                 |           |\n",
      "|    fps                | 514       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.446    |\n",
      "|    explained_variance | -7.63e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 2.14      |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | 101       |\n",
      "| time/                 |           |\n",
      "|    fps                | 515       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.612    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | 1.57      |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 103      |\n",
      "|    ep_rew_mean        | 103      |\n",
      "| time/                 |          |\n",
      "|    fps                | 515      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.607   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 1.36     |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 106      |\n",
      "|    ep_rew_mean        | 106      |\n",
      "| time/                 |          |\n",
      "|    fps                | 516      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.603   |\n",
      "|    explained_variance | 6.26e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | 1.51     |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 111      |\n",
      "|    ep_rew_mean        | 111      |\n",
      "| time/                 |          |\n",
      "|    fps                | 516      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.635   |\n",
      "|    explained_variance | 2.47e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 1.64     |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 115      |\n",
      "|    ep_rew_mean        | 115      |\n",
      "| time/                 |          |\n",
      "|    fps                | 517      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.554   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 1.76     |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 120      |\n",
      "|    ep_rew_mean        | 120      |\n",
      "| time/                 |          |\n",
      "|    fps                | 518      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.583   |\n",
      "|    explained_variance | 7.11e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 1.99     |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 121      |\n",
      "|    ep_rew_mean        | 121      |\n",
      "| time/                 |          |\n",
      "|    fps                | 518      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.633   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | 1.4      |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 123      |\n",
      "|    ep_rew_mean        | 123      |\n",
      "| time/                 |          |\n",
      "|    fps                | 519      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.515   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | 1.86     |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 125      |\n",
      "|    ep_rew_mean        | 125      |\n",
      "| time/                 |          |\n",
      "|    fps                | 519      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.328   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 2.53     |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 127      |\n",
      "|    ep_rew_mean        | 127      |\n",
      "| time/                 |          |\n",
      "|    fps                | 519      |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 17000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.471   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | 1.18     |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 127      |\n",
      "|    ep_rew_mean        | 127      |\n",
      "| time/                 |          |\n",
      "|    fps                | 519      |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 17500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.516   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | 0.765    |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 129      |\n",
      "|    ep_rew_mean        | 129      |\n",
      "| time/                 |          |\n",
      "|    fps                | 519      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.427   |\n",
      "|    explained_variance | 6.2e-06  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 1.75     |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 132      |\n",
      "|    ep_rew_mean        | 132      |\n",
      "| time/                 |          |\n",
      "|    fps                | 519      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.507   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | 1.4      |\n",
      "|    value_loss         | 10.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 135      |\n",
      "|    ep_rew_mean        | 135      |\n",
      "| time/                 |          |\n",
      "|    fps                | 520      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.309   |\n",
      "|    explained_variance | 2.98e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 1.57     |\n",
      "|    value_loss         | 10.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 139      |\n",
      "|    ep_rew_mean        | 139      |\n",
      "| time/                 |          |\n",
      "|    fps                | 520      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.351   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.484    |\n",
      "|    value_loss         | 10.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 142      |\n",
      "|    ep_rew_mean        | 142      |\n",
      "| time/                 |          |\n",
      "|    fps                | 520      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 38       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.35    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 0.793    |\n",
      "|    value_loss         | 10.6     |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch=[\n",
    "      dict(vf=[64, 64], pi=[64, 64]), # network architectures for actor/critic\n",
    "    ],\n",
    "    ortho_init=True, # Orthogonal initialization,\n",
    "    activation_fn=nn.Tanh,\n",
    ")\n",
    "\n",
    "hyperparams = dict(\n",
    "    n_steps=5,\n",
    "    learning_rate=7e-4,\n",
    "    gamma=0.9999, # discount factor\n",
    "    gae_lambda=1.0, # Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "                    # Equivalent to classic advantage when set to 1.\n",
    "    max_grad_norm=0.5, # The maximum value for the gradient clipping\n",
    "    ent_coef=0.0, # Entropy coefficient for the loss calculation\n",
    ")\n",
    "\n",
    "model = A2C(\"MlpPolicy\", \"CartPole-v1\", seed=8, verbose=1, **hyperparams).learn(budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T10:21:59.947074Z",
     "start_time": "2021-04-09T10:21:53.543063Z"
    },
    "id": "7YHvzTJArTGU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:175.36 +/- 22.50\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=50, deterministic=True)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iL_G9DurUV75"
   },
   "source": [
    "Hint - Recommended Hyperparameter Range\n",
    "\n",
    "```python\n",
    "gamma = trial.suggest_float(\"gamma\", 0.9, 0.99999, log=True)\n",
    "max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
    "gae_lambda = trial.suggest_float(\"gae_lambda\", 0.8, 0.999, log=True)\n",
    "# from 2**3 = 8 to 2**10 = 1024\n",
    "n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
    "learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
    "ent_coef = trial.suggest_float(\"ent_coef\", 0.00000001, 0.1, log=True)\n",
    "ortho_init = trial.suggest_categorical(\"ortho_init\", [False, True])\n",
    "# tiny: {\"pi\": [64], \"vf\": [64]}\n",
    "# default: {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
    "net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"default\"])\n",
    "activation_fn = trial.suggest_categorical(\"activation_fn\", [nn.Tanh, nn.ReLU])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCbep6z1h3D1"
   },
   "source": [
    "Simple example of hyperparameter tuning: https://github.com/optuna/optuna/blob/master/examples/rl/sb3_simple.py\n",
    "\n",
    "Complete example: https://github.com/DLR-RM/rl-baselines3-zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yUeYnfJVpB2"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "What we have seen in this notebook:\n",
    "- SB3 101\n",
    "- Gym wrappers to modify the env\n",
    "- SB3 callbacks to access the RL agent\n",
    "- multiprocessing to speedup training\n",
    "- the importance of good hyperparameters\n",
    "- more complete tutorial: https://github.com/araffin/rl-tutorial-jnrr19\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-gqIPXqV7zZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "rlvs_hands_on_sb3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
